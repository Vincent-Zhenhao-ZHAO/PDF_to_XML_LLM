[
    {
        "element_id": "09dcfb1bc6937156ae3a993dc70a868e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        574.3,
                        100.1
                    ],
                    [
                        574.3,
                        121.7
                    ],
                    [
                        1076.0,
                        121.7
                    ],
                    [
                        1076.0,
                        100.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8015,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "f9ff63fdfb4cb802c81483aa62447102",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.3,
                        170.0
                    ],
                    [
                        101.3,
                        345.8
                    ],
                    [
                        275.8,
                        345.8
                    ],
                    [
                        275.8,
                        170.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.64496,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "ELSEVIER",
        "type": "Image"
    },
    {
        "element_id": "35cb663a03ca4c6233418af682f45d24",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        634.4,
                        171.2
                    ],
                    [
                        634.4,
                        197.6
                    ],
                    [
                        1026.4,
                        197.6
                    ],
                    [
                        1026.4,
                        171.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.30239,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "Contents lists available at ScienceDirect",
        "type": "NarrativeText"
    },
    {
        "element_id": "ed9e56ed329eaed456c9641e24db47ee",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        520.7,
                        236.5
                    ],
                    [
                        520.7,
                        277.2
                    ],
                    [
                        1136.2,
                        277.2
                    ],
                    [
                        1136.2,
                        236.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.32466,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "Computers in Biology and Medicine",
        "type": "Title"
    },
    {
        "element_id": "8abe12ac6d964868a818385bd5d7604d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        524.0,
                        327.7
                    ],
                    [
                        524.0,
                        350.6
                    ],
                    [
                        1133.4,
                        350.6
                    ],
                    [
                        1133.4,
                        327.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.32056,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1,
            "parent_id": "ed9e56ed329eaed456c9641e24db47ee"
        },
        "text": "journal homepage: www.elsevier.com/locate/compbiomed",
        "type": "NarrativeText"
    },
    {
        "element_id": "2b2376262f6c312db1d52bc9b5fb4abc",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1404.0,
                        170.0
                    ],
                    [
                        1404.0,
                        217.0
                    ],
                    [
                        1530.0,
                        217.0
                    ],
                    [
                        1530.0,
                        170.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "= eee \u2018omputers in Biology and Medicine",
        "type": "Title"
    },
    {
        "element_id": "2a2abf5f7947e0a1d000d4917620cec8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1433.1,
                        275.6
                    ],
                    [
                        1433.1,
                        320.2
                    ],
                    [
                        1502.7,
                        320.2
                    ],
                    [
                        1502.7,
                        275.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.33546,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1,
            "parent_id": "2b2376262f6c312db1d52bc9b5fb4abc"
        },
        "text": "EE a",
        "type": "NarrativeText"
    },
    {
        "element_id": "a80a63e05827cf6e3cd673ce0deab6b7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        100.7,
                        465.5
                    ],
                    [
                        100.7,
                        498.9
                    ],
                    [
                        1161.3,
                        498.9
                    ],
                    [
                        1161.3,
                        465.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84057,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "AGNet: Automatic generation network for skin imaging reports",
        "type": "Title"
    },
    {
        "element_id": "a325ed7cb06944a0fceace31a8913a9e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1396.8,
                        412.4
                    ],
                    [
                        1396.8,
                        492.0
                    ],
                    [
                        1466.0,
                        492.0
                    ],
                    [
                        1466.0,
                        412.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.45629,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "Check for updates",
        "type": "Image"
    },
    {
        "element_id": "710e680c539135c8952b611094c8700a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.6,
                        534.9
                    ],
                    [
                        103.6,
                        600.4
                    ],
                    [
                        1264.2,
                        600.4
                    ],
                    [
                        1264.2,
                        534.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7456,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "Fan Wu*\u00b0', Haiqiong Yang\u201d\u2019', Linlin Peng\u2019, Zongkai Lian\u2018, Mingxin Li>, Gang Qu?, Shancheng Jiang\u2019\u00b0, Yu Han\u201d",
        "type": "NarrativeText"
    },
    {
        "element_id": "cdee65f2d273a52b4147770b4831a5b2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        93.3,
                        627.8
                    ],
                    [
                        93.3,
                        647.0
                    ],
                    [
                        1089.5,
                        647.0
                    ],
                    [
                        1089.5,
                        627.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.63982,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "* The School of Intelligent Systems Engineering, Sun Yat-Sen University, No. 135, Xingang Xi Road, Guangzhou, 510275, PR China",
        "type": "ListItem"
    },
    {
        "element_id": "ee8b277282e6b366cf2a1618fdf18e3a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        106.9,
                        651.2
                    ],
                    [
                        106.9,
                        671.0
                    ],
                    [
                        808.7,
                        671.0
                    ],
                    [
                        808.7,
                        651.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74506,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "> Dalian Dermatosis Hospital, No. 788, Changjiang Road, Shahekou District, Dalian, PR China",
        "type": "ListItem"
    },
    {
        "element_id": "3a2f78102a072fcb27f9684a9211013f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        99.5,
                        675.9
                    ],
                    [
                        99.5,
                        694.6
                    ],
                    [
                        862.5,
                        694.6
                    ],
                    [
                        862.5,
                        675.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76463,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "\u00a9 Guangdong Provincial Key Laboratory of Fire Science and Technology, Guangzhou, 510006, China",
        "type": "ListItem"
    },
    {
        "element_id": "727af2c68e2c358aa12098ac9d851cff",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.9,
                        768.5
                    ],
                    [
                        103.9,
                        786.9
                    ],
                    [
                        324.3,
                        786.9
                    ],
                    [
                        324.3,
                        768.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.43809,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "ARTICLE INFO",
        "type": "NarrativeText"
    },
    {
        "element_id": "779d1cf1512eb321af53e58ed10e57a4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        559.1,
                        768.6
                    ],
                    [
                        559.1,
                        787.0
                    ],
                    [
                        720.6,
                        787.0
                    ],
                    [
                        720.6,
                        768.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.51636,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "ABSTRACT",
        "type": "NarrativeText"
    },
    {
        "element_id": "c38a832ecb3362d6e36c572b3c109e69",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        109.1,
                        815.8
                    ],
                    [
                        109.1,
                        957.0
                    ],
                    [
                        326.3,
                        957.0
                    ],
                    [
                        326.3,
                        815.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.5736,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "Index Terms: Attention mechanism Deep learning Image caption Medical imaging Skin imaging report generation",
        "type": "NarrativeText"
    },
    {
        "element_id": "8fd290d46637cf3dd8200fa0d86ee2dc",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        562.5,
                        820.3
                    ],
                    [
                        562.5,
                        1214.2
                    ],
                    [
                        1550.0,
                        1214.2
                    ],
                    [
                        1550.0,
                        820.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9354,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "Medical imaging has been increasingly adopted in the process of medical diagnosis, especially for skin diseases, where diagnoses based on skin pathology are extremely accurate. The diagnostic reports of skin pathology im- ages has the distinguishing features of extreme repetitiveness and rigid formatting. However, reports written by inexperienced radiologists and pathologists can have a high error rate, and even experienced clinicians can find the reporting task both tedious and time-consuming. To address this challenge, this paper studies the automatic generation of diagnostic reports based on images of skin pathologies. A novel deep learning-based image caption framework named the automatic generation network (AGNet), which is an effective network for the automatic generation of skin imaging reports, is proposed. The proposed AGNet consists of four parts: (1) the image model that extracts features and classifies images; (2) the language model that codes data and generates words using comprehensible language; (3) the attention module that connects the \u201ctail\u201d of the image model and the \u201chead\u201d of the language model, and computes the relationship between images and captions; (4) the embedding and la- beling module that processes the input caption data. In case study, The AGNet is verified on a skin pathological image dataset and compared with several state-of-the-art models. The results show that the AGNet achieves the highest scores of the evaluation metrics of image caption among all comparison models, demonstrating the promising performance of the proposed method.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5547ec45a27b559ad39a72c1343fd7c8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.1,
                        1302.5
                    ],
                    [
                        105.1,
                        1322.3
                    ],
                    [
                        271.3,
                        1322.3
                    ],
                    [
                        271.3,
                        1302.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85279,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "1. Introduction",
        "type": "Title"
    },
    {
        "element_id": "1af62514d0689236fd63292b94631649",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.8,
                        1360.7
                    ],
                    [
                        102.8,
                        1698.7
                    ],
                    [
                        803.7,
                        1698.7
                    ],
                    [
                        803.7,
                        1360.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95518,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1,
            "parent_id": "5547ec45a27b559ad39a72c1343fd7c8"
        },
        "text": "Skin diseases have the characteristics of a wide range, various cate- gories, and a long treatment cycle. In most hospitals, especially those that are not specialized and are of small or medium size, the derma- tology department is understaffed, and there is a high probability of misdiagnosis. Namely, a number of skin diseases can be easily diagnosed by the naked-eye method, but related research has indicated that, with the help of dermatoscopy, experts could achieve a sensitivity of 90% and a specificity of 59% in the diagnosis of skin lesions. Still, most derma- tologists can reach a sensitivity of only about 62%, which is far below that of experts. Moreover, it takes about 5-10 min to enter each image\u2019s results into a computer, which in total consumes an inordinate amount of working time. In addition, writing imaging reports is unpleasant for",
        "type": "NarrativeText"
    },
    {
        "element_id": "5aa245987cd676a58405340b35323be7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        852.6,
                        1302.3
                    ],
                    [
                        852.6,
                        1670.3
                    ],
                    [
                        1555.8,
                        1670.3
                    ],
                    [
                        1555.8,
                        1302.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95172,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1,
            "parent_id": "5547ec45a27b559ad39a72c1343fd7c8"
        },
        "text": "both experienced and inexperienced clinicians. Therefore, much recent research has been devoted to the application of medical image pro- cessing, traditional pattern recognition, modern machine learning for medical images, and advanced artificial intelligence technology to the extraction of key information implied by detection results for the pur- pose of achieving automatic diagnoses of skin lesions [1,2]. Recent research findings have considerably aided dermatologists and general practitioners in processing image data faster and providing more reliable diagnoses, and there are few doubts that modern diagnostic technology will eventually achieve its full potential. However, most computer-aided diagnosis methods have been confined to pathological image classifi- cation based on a single image source for detecting skin disease and extraction of the pathological area.",
        "type": "NarrativeText"
    },
    {
        "element_id": "125baa2d2c372721ee2369f4cee63be0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        888.6,
                        1680.9
                    ],
                    [
                        888.6,
                        1700.1
                    ],
                    [
                        1547.5,
                        1700.1
                    ],
                    [
                        1547.5,
                        1680.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79173,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1,
            "parent_id": "5547ec45a27b559ad39a72c1343fd7c8"
        },
        "text": "Furthermore, the inability to interpret model predictions in",
        "type": "NarrativeText"
    },
    {
        "element_id": "16f40628d6c1573ee9c77631dd636dc3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.5,
                        1758.6
                    ],
                    [
                        102.5,
                        1888.9
                    ],
                    [
                        1548.5,
                        1888.9
                    ],
                    [
                        1548.5,
                        1758.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.6249,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1,
            "parent_id": "5547ec45a27b559ad39a72c1343fd7c8"
        },
        "text": "* Corresponding author. The School of Intelligent Systems Engineering, Sun Yat-Sen University, No. 135, Xingang Xi Road, Guangzhou, 510275, PR China. ** Corresponding author. The School of Intelligent Systems Engineering, Sun Yat-Sen University, No. 135, Xingang Xi Road, Guangzhou, 510275, PR China. E-mail addresses: wufan55@mail2.sysu.edu.cn (F. Wu), 396244148@qq.com (H. Yang), haiguaipll|@163.com (L. Peng), lianzk@mail2.sysu.edu.cn (Z. Lian), Imxpfbyy@sina.com (M. Li), 1638654878@qq.com (G. Qu), jiangshch3@mail.sysu.edu.cn (S. Jiang), hanyu25@mail.sysu.edu.cn, hanyu25@mail.sysu.edu.cn (Y. Han).",
        "type": "ListItem"
    },
    {
        "element_id": "37e5e9d42cdabdce600ee626c7ab38d5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        114.3,
                        1891.9
                    ],
                    [
                        114.3,
                        1911.1
                    ],
                    [
                        723.3,
                        1911.1
                    ],
                    [
                        723.3,
                        1891.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.65851,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1,
            "parent_id": "5547ec45a27b559ad39a72c1343fd7c8"
        },
        "text": "1 These two authors indicated by contributed equally to this work.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a187befd12a65e7e0895c12184347f88",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.8,
                        1939.5
                    ],
                    [
                        103.8,
                        1960.0
                    ],
                    [
                        581.4,
                        1960.0
                    ],
                    [
                        581.4,
                        1939.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74903,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1
        },
        "text": "https://doi.org/10.1016/j.compbiomed.2021.105037",
        "type": "Title"
    },
    {
        "element_id": "42252bbfe8fb288184f9fe1f46bfb453",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        89.6,
                        1962.6
                    ],
                    [
                        89.6,
                        2032.7
                    ],
                    [
                        1011.2,
                        2032.7
                    ],
                    [
                        1011.2,
                        1962.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.54241,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 1,
            "parent_id": "a187befd12a65e7e0895c12184347f88"
        },
        "text": "Received 29 June 2021; Received in revised form 11 November 2021; Accepted 11 November 2021 Available online 14 November 2021 0010-4825/\u00a9 2021 Elsevier Ltd. All rights reserved.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "19d4d395b7d4939214c13724e405b97a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.5,
                        98.6
                    ],
                    [
                        102.5,
                        120.5
                    ],
                    [
                        194.7,
                        120.5
                    ],
                    [
                        194.7,
                        98.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74251,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "F. Wuet al.",
        "type": "Header"
    },
    {
        "element_id": "f198b23d24a8f78199215e94b5dce64b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        120.6,
                        151.4
                    ],
                    [
                        120.6,
                        436.8
                    ],
                    [
                        797.6,
                        436.8
                    ],
                    [
                        797.6,
                        151.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89825,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "aaa Diagnostic Label report",
        "type": "Image"
    },
    {
        "element_id": "f004744c610782a63af0ff9bdbbd9122",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        279.0,
                        483.8
                    ],
                    [
                        279.0,
                        504.5
                    ],
                    [
                        627.2,
                        504.5
                    ],
                    [
                        627.2,
                        483.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8121,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "Fig. 1. Overview of the AGNet model.",
        "type": "FigureCaption"
    },
    {
        "element_id": "2230e03d9af8efb568e57b8c939bea9a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.8,
                        544.0
                    ],
                    [
                        104.8,
                        769.7
                    ],
                    [
                        805.6,
                        769.7
                    ],
                    [
                        805.6,
                        544.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95528,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "semantically and visually meaningful ways is a well-known shortcoming of most computer-aided diagnosis methods, which makes these methods difficult to use in actual diagnosis. The intelligent extraction of medical information is still in its infancy, and high-quality and properly labeled images of skin disease samples are scarce, especially in the case of his- topathological images that must be collected by a microscope. Thus, it is challenging for practitioners to exploit most deep learning-based architectures.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d90037b88786fe9a3e8915012d13001c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.2,
                        778.1
                    ],
                    [
                        105.2,
                        1378.4
                    ],
                    [
                        807.8,
                        1378.4
                    ],
                    [
                        807.8,
                        778.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94553,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "Aiming at the mentioned limitations, this paper proposes a method that can automatically generate diagnostic reports. The structure of the proposed method is presented in Fig. 1. The main objective is to answer three leading questions. First, how it can be ensured that a generated report\u2019s accuracy meets the standards required for medical images. Second, how high calculation speeds can be achieved so that the model completes the calculation and gives feedback within a short time span. Third, what is the best way to visualize the judgment basis, that is, how text generated by the model reflects appropriate areas in an image. These questions are addressed by proposing a deep learning-based image caption framework named the automatic generation network (AGNet) through combining a novel image model and a language model. With the aim of visualizing the judgment basis, a novel attention mod- ule, namely the similarity-attention mechanism, which bridges the gap between the image model and the language model and calculates the mapping relationship between text and image data efficiently, is designed. In addition, a new embedding and labeling module is intro- duced to embed the caption information into the language model as well as to merge the label information into the caption data. Proceed from the questions raised above, Our main contributions can be summarized into three points.",
        "type": "NarrativeText"
    },
    {
        "element_id": "30ab36a62e58cc565645eeba31ae7cf5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        134.4,
                        1414.5
                    ],
                    [
                        134.4,
                        1553.0
                    ],
                    [
                        803.0,
                        1553.0
                    ],
                    [
                        803.0,
                        1414.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92645,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "(1) We proposed a novel deep learning-based image caption frame- work, namely AGNet (Automatic Generation Network), through combining a novel image model and language model which can automatically generate diagnosis on the basis of input skin pathological image.",
        "type": "ListItem"
    },
    {
        "element_id": "64d53afe2e97b9dab4da3a46177c90a1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        134.7,
                        1558.4
                    ],
                    [
                        134.7,
                        1816.2
                    ],
                    [
                        809.1,
                        1816.2
                    ],
                    [
                        809.1,
                        1558.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95149,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "(2) With the purpose of visualizing the \u201cjudgment basis\u201d to provide visual explanations for the skin disease specialists, we design a novel attention module, namely \u201csimilarity-attention\u201d mecha- nism, that bridges the gap between the image model and the language model and calculates the mapping relationship between text and image data more efficiently. We also propose a new \u201cembedding & labeling\u201d module to embed the caption informa- tion into the language model as well as merge the label infor- mation into the caption data.",
        "type": "ListItem"
    },
    {
        "element_id": "9630ccea9e57bc2aff66fe85d56ea71e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        131.3,
                        1820.5
                    ],
                    [
                        131.3,
                        1930.4
                    ],
                    [
                        800.6,
                        1930.4
                    ],
                    [
                        800.6,
                        1820.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92978,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "(3) Extensive experimental results show that AGNet outperforms and achieves the best over several well-known methods on medical image caption with the proposed model needing a much fewer training time.",
        "type": "ListItem"
    },
    {
        "element_id": "789383acaed6f7c2cff547a3988eb986",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.8,
                        1967.0
                    ],
                    [
                        102.8,
                        2047.5
                    ],
                    [
                        802.0,
                        2047.5
                    ],
                    [
                        802.0,
                        1967.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93014,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 introduces the proposed AGNet method. Section 4 presents the experimental results. Finally, Section 5 gives a brief",
        "type": "NarrativeText"
    },
    {
        "element_id": "4eb176fb7ec8fa88048c2912b64af4a1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1128.5,
                        101.5
                    ],
                    [
                        1128.5,
                        118.8
                    ],
                    [
                        1553.8,
                        118.8
                    ],
                    [
                        1553.8,
                        101.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.72205,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "7931ae8833b1d8d049ce8fc0dc223734",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.1,
                        154.9
                    ],
                    [
                        851.1,
                        173.4
                    ],
                    [
                        950.3,
                        173.4
                    ],
                    [
                        950.3,
                        154.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.58864,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2,
            "parent_id": "4eb176fb7ec8fa88048c2912b64af4a1"
        },
        "text": "summary.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ae54a3f9d65422ed6e61ba114a4c397a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        852.8,
                        211.9
                    ],
                    [
                        852.8,
                        231.2
                    ],
                    [
                        1027.1,
                        231.2
                    ],
                    [
                        1027.1,
                        211.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74224,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2,
            "parent_id": "4eb176fb7ec8fa88048c2912b64af4a1"
        },
        "text": "2. Related work",
        "type": "Title"
    },
    {
        "element_id": "6102b0ad78789c9c69ec65a54b2567f3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.7,
                        270.3
                    ],
                    [
                        853.7,
                        289.3
                    ],
                    [
                        1099.2,
                        289.3
                    ],
                    [
                        1099.2,
                        270.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.53143,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2,
            "parent_id": "4eb176fb7ec8fa88048c2912b64af4a1"
        },
        "text": "2.1. Attention mechanism",
        "type": "Title"
    },
    {
        "element_id": "a57d308a29b32595555fac2a4a9ae0ce",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.2,
                        328.7
                    ],
                    [
                        851.2,
                        872.2
                    ],
                    [
                        1550.7,
                        872.2
                    ],
                    [
                        1550.7,
                        328.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95077,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2,
            "parent_id": "6102b0ad78789c9c69ec65a54b2567f3"
        },
        "text": "The attention mechanism [3,4], which was derived from human intuition, has been widely applied to medical image analysis [5,6] and has yielded significant performance improvements in various sequence-learning tasks. The attention mechanism calculates an importance score for each candidate vector first, then normalizes the scores to weights using the soft-max function, and finally, applies the obtained weights to the candidates to generate the attention result that represents a weighted average vector [7]. The attention mechanisms can be of various types, including the spatial and channel-wise attention [8], adaptive attention [9], stacked attention [10], multi-level attention [11], top-down visual attention [12], multi-head attention, and self-attention [13]. The well-known and significant work of Vaswani et al. [13] has shown that the state-of-the-art results in machine trans- lation can be achieved by using only the self-attention mechanism. Several recent studies have extended the idea of employing the self-attention mechanism to computer vision tasks [14\u201416], which was our main inspiration to use self-attention in image captioning to model relationships between the objects of interest and the corresponding words in a diagnostic report on the image.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6675cd9589c6616337602a184b85fc23",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        852.2,
                        910.1
                    ],
                    [
                        852.2,
                        929.1
                    ],
                    [
                        1061.7,
                        929.1
                    ],
                    [
                        1061.7,
                        910.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.66086,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2,
            "parent_id": "4eb176fb7ec8fa88048c2912b64af4a1"
        },
        "text": "2.2. Image captioning",
        "type": "Title"
    },
    {
        "element_id": "e72ffe260ab787add22d070695d55ad5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.8,
                        968.8
                    ],
                    [
                        851.8,
                        1597.5
                    ],
                    [
                        1552.1,
                        1597.5
                    ],
                    [
                        1552.1,
                        968.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94768,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2,
            "parent_id": "6675cd9589c6616337602a184b85fc23"
        },
        "text": "Earlier methods for image captioning were based on rules that generated slotted caption templates first and then filled the slots with the result of object detection, attribute prediction, and scene recogni- tion. However, more recent methods have been neural-based [17-23], and most of them use the deep encoder-decoder framework inspired by the neural-based machine translation. For instance, an end-to-end framework, where a convolutional neural network (CNN) is used for encoding of the image-to-feature vector and followed by a long short-term memory (LSTM) network [24] used for decoding to a caption, has been proposed. In Ref. [7], the spatial attention mechanism based on a CNN feature map was used to incorporate visual context. The latest studies have highlighted that adding an attention module to the model can significantly improve model performance. Chen et al. [8] proposed a spatial and channel-wise attention model. Liu et al. [9] introduced an adaptive attention mechanism to determine when visual attention should be activated. Recent developments include integrating more complex information, such as information on objects, attributes, and relationships, to obtain better descriptions [12,25-28] by discovering the causal relationship between objects in a particular image [29]. Furthermore, considering the good performance and universality of the LSTM in interpreting image data, the proposed method uses the LSTM as a skeleton of the language model for caption generation.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f8bbd9b02ddf26551f32cd581b5af38f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        850.3,
                        1635.6
                    ],
                    [
                        850.3,
                        1656.1
                    ],
                    [
                        1228.2,
                        1656.1
                    ],
                    [
                        1228.2,
                        1635.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.6399,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2,
            "parent_id": "4eb176fb7ec8fa88048c2912b64af4a1"
        },
        "text": "2.3. Textual labeling of medical images",
        "type": "Title"
    },
    {
        "element_id": "12f8ef3ebea8f3c2308ab608d1232829",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.9,
                        1692.4
                    ],
                    [
                        854.9,
                        2033.6
                    ],
                    [
                        1553.7,
                        2033.6
                    ],
                    [
                        1553.7,
                        1692.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95454,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2,
            "parent_id": "f8bbd9b02ddf26551f32cd581b5af38f"
        },
        "text": "Previously, there have been several methods that perform positively in similar tasks. For instance, Tian et al. [30] developed a descriptive review framework based on the Latent Dirichlet Allocation (LDA), and studied the application of deep learning-based classification, object detection, segmentation, and image generation in the field of medical images. Chai et al. [31] designed a multi-branch neural network (MB-NN) model that can automatically extract important areas of im- ages and obtain domain knowledge features. However, the objective of this study is to generate a diagnostic report based on the attention re- gions, while the work of Chai et al. has failed to provide a practical method to illustrate the relationship between text and image data. In view of this, we eventually turn to the most suitable method.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bd2bd8b2f3b930005cd44f9fce5040f2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        870.3,
                        2042.8
                    ],
                    [
                        870.3,
                        2062.5
                    ],
                    [
                        1554.8,
                        2062.5
                    ],
                    [
                        1554.8,
                        2042.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79955,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 2,
            "parent_id": "f8bbd9b02ddf26551f32cd581b5af38f"
        },
        "text": "There has been an increasing trend in applying both machine",
        "type": "NarrativeText"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "57ccb21f4bdb1046a5e4e0af09ff8579",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.4,
                        98.8
                    ],
                    [
                        101.4,
                        120.2
                    ],
                    [
                        193.7,
                        120.2
                    ],
                    [
                        193.7,
                        98.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73278,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "F. Wuet al.",
        "type": "Header"
    },
    {
        "element_id": "c70eb0a8827c98aa1e8b5f12c6bd88f8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1129.6,
                        101.5
                    ],
                    [
                        1129.6,
                        118.7
                    ],
                    [
                        1549.7,
                        118.7
                    ],
                    [
                        1549.7,
                        101.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76223,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "245976c35dea9f0bb0d0b4e3cd5c36f2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        184.1,
                        160.6
                    ],
                    [
                        184.1,
                        467.3
                    ],
                    [
                        1441.2,
                        467.3
                    ],
                    [
                        1441.2,
                        160.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91427,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "Training dataset HES Image model : Embedding & Labeling <~_\u2014 ae Seen : oF \u2018Similarity | . : $3 \u2018attention + q : > word 3 3 \u2014\u2014\u2014 , $3 2s Language model g*",
        "type": "Image"
    },
    {
        "element_id": "95eef2e32962e41ad2277c82a5752286",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        241.5,
                        505.3
                    ],
                    [
                        241.5,
                        524.3
                    ],
                    [
                        358.8,
                        524.3
                    ],
                    [
                        358.8,
                        505.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.6137,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "Testing dataset",
        "type": "FigureCaption"
    },
    {
        "element_id": "8a7f3067895d48c0f3c95b42fce80fd2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        97.6,
                        553.0
                    ],
                    [
                        97.6,
                        625.6
                    ],
                    [
                        1548.2,
                        625.6
                    ],
                    [
                        1548.2,
                        553.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84763,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "Fig. 2. Schematic illustration of the AGNet model. The green lines are shared by the training and testing process. The blue and green lines with arrows represent the data flow in the testing process; meanwhile, the red and green lines with arrows represent the data flow in the training process. (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)",
        "type": "NarrativeText"
    },
    {
        "element_id": "bfd41e3ba27154cde6448461e37a6c36",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.5,
                        665.6
                    ],
                    [
                        103.5,
                        1037.6
                    ],
                    [
                        804.3,
                        1037.6
                    ],
                    [
                        804.3,
                        665.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95631,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "learning and deep learning to medical images [32-34], attaching text to medical images [18,35-37], and processing mining [38-40]. As for the first two tasks, the target text is either fully structured or semi-structured (e.g., tags and templates) rather than being a natural text. Kisilev et al. [41] constructed a pipeline for predicting the attributes of medical im- ages. Shin et al. [17] adopted a CNN-RNN framework to predict tags (e. g., locations and severities) of chest X-ray images. Reference [36] has most closely related to this study, but in Ref. [36], the authors aimed to generate semi-structured pathology reports whose contents were restricted to five pre-defined topics. However, in practice, caption data seldom conform to such categorization, and collecting semi-structured reports is not practical, so models capable of learning from natural re- ports are urgently needed.",
        "type": "NarrativeText"
    },
    {
        "element_id": "56c47e743c9aebed4968aaf994a08756",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        865.0,
                        683.5
                    ],
                    [
                        865.0,
                        1023.0
                    ],
                    [
                        1531.4,
                        1023.0
                    ],
                    [
                        1531.4,
                        683.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92572,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "Filter concatenation 3x3 convolutions 5x5 convolutions 1x1 convolutions 1x1 convolutions f f t 1x1 convolutions 1x1 convolutions 3x3 max pooling Previous layer",
        "type": "Image"
    },
    {
        "element_id": "ebfd507f5154b29fa7c9bc3d0218bdc7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1003.6,
                        1049.6
                    ],
                    [
                        1003.6,
                        1069.3
                    ],
                    [
                        1393.3,
                        1069.3
                    ],
                    [
                        1393.3,
                        1049.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83523,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "Fig. 3. Overview of the inception module.",
        "type": "FigureCaption"
    },
    {
        "element_id": "ee8f7992dd1177b8c7c40daf2d13f873",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.2,
                        1073.0
                    ],
                    [
                        104.2,
                        1095.8
                    ],
                    [
                        274.5,
                        1095.8
                    ],
                    [
                        274.5,
                        1073.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82094,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "3. Methodology",
        "type": "Title"
    },
    {
        "element_id": "24f1c1c77b748db259d37c6043a68638",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        850.3,
                        1111.3
                    ],
                    [
                        850.3,
                        1132.1
                    ],
                    [
                        1021.8,
                        1132.1
                    ],
                    [
                        1021.8,
                        1111.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.44849,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "3.2. Image model",
        "type": "Title"
    },
    {
        "element_id": "97f2304b509c9795543413ff61ca86ba",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.4,
                        1132.2
                    ],
                    [
                        105.4,
                        1152.8
                    ],
                    [
                        305.7,
                        1152.8
                    ],
                    [
                        305.7,
                        1132.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71875,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "3.1. AGNet overview",
        "type": "Title"
    },
    {
        "element_id": "786c24b3548711bf3cbb09683be01acd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.0,
                        1189.1
                    ],
                    [
                        104.0,
                        1530.5
                    ],
                    [
                        803.1,
                        1530.5
                    ],
                    [
                        803.1,
                        1189.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95758,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3,
            "parent_id": "97f2304b509c9795543413ff61ca86ba"
        },
        "text": "The proposed AGNet model consists of an image model, which in- cludes a refine module consisting of the feature extraction network and multi-label classification (MLC) network, a similarity-attention module, and an embedding and labeling module, and a language model. Each data pair used in this study includes an image and its description and category label. The image of a data pair is first fed to the feature extraction network, from which the extracted feature vector is obtained, which is further used by the MLC network to predict the relevant tag. The MLC network aims to use the category labels for supervised learning; it reverses the gradient from the feature extraction network so that it can guide the network and thereby correct the learning from feature extraction.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2d1113f33a07b77ab997a1f42482a3ee",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.7,
                        1170.2
                    ],
                    [
                        854.7,
                        1492.6
                    ],
                    [
                        1553.1,
                        1492.6
                    ],
                    [
                        1553.1,
                        1170.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95433,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3,
            "parent_id": "97f2304b509c9795543413ff61ca86ba"
        },
        "text": "The image model, which is the first model in the AGNet model, has two primary functions: to extract meaningful feature maps for further processing by the similarity-attention module and to predict a reliable disease category label, which is used as an input of the embedding and labeling module. To perform both functions in a single pass, the image model incorporates a feature extraction network followed by an MLC network. It should be noted that the image model is pre-trained sepa- rately using known disease category labels. More specifically, for a given image I, first, its feature {v,}{_, \u00a9 R'*\u2122 is extracted by the feature extraction network, and then {v; Yaa is fed to the MLC network, thereby generating a distribution over all of L tags as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "673009ffd62fb82d57cbeca3b7c70bb5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.7,
                        1537.8
                    ],
                    [
                        103.7,
                        1996.8
                    ],
                    [
                        806.2,
                        1996.8
                    ],
                    [
                        806.2,
                        1537.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95213,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3,
            "parent_id": "97f2304b509c9795543413ff61ca86ba"
        },
        "text": "Once a feature has been correctly extracted, the refine module pro- cesses it and outputs the refined feature, which fits the caption data better in terms of dimension. In the embedding and labeling module, the image description text first undergoes the one-hot encoding based on the dictionary built from the dataset; then, the obtained data are fed to the embedding and labeling module to generate the embedded matrix with label information. Next, the similarity-attention module treats the embedded matrix and the refined feature as the query object and the key-value object, respectively, and calculates the similarity between them to obtain the attention value of the image feature. The similarity- attention module output and the word-embedding matrix are then fed to the language model via the LSTM [24], which underlies the generation of words in the next time step. Termination of the presented process is controlled by the language model. The terminator output of the LSTM completes the task of generating a diagnostic report. The illustration of the described process is shown in Fig. 2.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b876fd33e6de7a387816ef695965b7a8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        852.9,
                        1513.7
                    ],
                    [
                        852.9,
                        1541.7
                    ],
                    [
                        1557.1,
                        1541.7
                    ],
                    [
                        1557.1,
                        1513.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.65124,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "Piprea (li = 1 | {vi} )ocexp(MLC:({vi }t-1))> @",
        "type": "Formula"
    },
    {
        "element_id": "0fd2f0d71240244829f43008527f6705",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.0,
                        1573.3
                    ],
                    [
                        855.0,
                        1652.8
                    ],
                    [
                        1553.1,
                        1652.8
                    ],
                    [
                        1553.1,
                        1573.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93508,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "where Piprea denotes the tags distribution of prediction; 1 \u20ac Risa tag vector, and it signifies the presence (or absence) of the i-th tag and corresponds to the MLC network\u2019s ith output.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5672af97f5ff62bc9f018de53b37fb16",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.1,
                        1661.3
                    ],
                    [
                        854.1,
                        2060.3
                    ],
                    [
                        1552.0,
                        2060.3
                    ],
                    [
                        1552.0,
                        1661.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95207,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 3
        },
        "text": "The AGNet employs GoogLeNet Inception v3 [42] as its image model. GoogLeNet has a modular structure (inception), and the first version of the inception module is shown in Fig. 3. GoogLeNet is heaped up by several inception modules, of which Inception v3 is of the third generation. The Inception v3 module enables two types of improve- ments, decomposing a large filter into an asymmetric structure and solving the problem of information loss by using a parallel structure. These improvements significantly reduce the number of required cal- culations, which accounts (in part) for the superior performance of the Inception v3 module. To simplify the process, the proposed model ex- tracts visual features in the final convolutional layer of the Inception v3 model and uses its last two fully-connected layers for the MLC network. Finally, the extracted feature and the label vector are used to generate text in the subsequent network layers.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "c02a3cd23b6c09cffdbb97cb192f9518",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.0,
                        99.6
                    ],
                    [
                        101.0,
                        119.6
                    ],
                    [
                        196.0,
                        119.6
                    ],
                    [
                        196.0,
                        99.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.69676,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "F. Wuet al.",
        "type": "Header"
    },
    {
        "element_id": "d7950a7e4b07e3bbc3d5d989a016a0aa",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        314.0,
                        156.0
                    ],
                    [
                        314.0,
                        198.0
                    ],
                    [
                        568.0,
                        198.0
                    ],
                    [
                        568.0,
                        156.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "c02a3cd23b6c09cffdbb97cb192f9518"
        },
        "text": ". Refined map",
        "type": "NarrativeText"
    },
    {
        "element_id": "4d60387c859c308278e4e495c1c9e418",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        202.7,
                        193.1
                    ],
                    [
                        202.7,
                        900.8
                    ],
                    [
                        709.0,
                        900.8
                    ],
                    [
                        709.0,
                        193.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84178,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": ". . Sigmoid Dense Reshape",
        "type": "Image"
    },
    {
        "element_id": "c78b5441ed7be00a91bb4b4fd0848c72",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        313.7,
                        912.1
                    ],
                    [
                        313.7,
                        951.0
                    ],
                    [
                        574.0,
                        951.0
                    ],
                    [
                        574.0,
                        912.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.46658,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "Feature map",
        "type": "Title"
    },
    {
        "element_id": "99f09e125169238cb6b9fa48fa4de8f7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        275.1,
                        980.9
                    ],
                    [
                        275.1,
                        1000.0
                    ],
                    [
                        627.5,
                        1000.0
                    ],
                    [
                        627.5,
                        980.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.41005,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "c78b5441ed7be00a91bb4b4fd0848c72"
        },
        "text": "Fig. 4. Overview of the refine module.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b25ed53ebf5e9af0e03b3c8e7c915049",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.4,
                        1062.5
                    ],
                    [
                        104.4,
                        1081.5
                    ],
                    [
                        176.1,
                        1081.5
                    ],
                    [
                        176.1,
                        1062.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.63841,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "Table 1",
        "type": "Title"
    },
    {
        "element_id": "695c47dec4320302b8b3bb24fd1b7636",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        108.9,
                        1089.7
                    ],
                    [
                        108.9,
                        1107.0
                    ],
                    [
                        498.6,
                        1107.0
                    ],
                    [
                        498.6,
                        1089.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.56763,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "b25ed53ebf5e9af0e03b3c8e7c915049"
        },
        "text": "Classification accuracies of the three models.",
        "type": "NarrativeText"
    },
    {
        "element_id": "26224ebb80036fc33433ba758a26048b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        106.5,
                        1112.9
                    ],
                    [
                        106.5,
                        1236.9
                    ],
                    [
                        807.3,
                        1236.9
                    ],
                    [
                        807.3,
                        1112.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91686,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "b25ed53ebf5e9af0e03b3c8e7c915049",
            "text_as_html": "<table><thead><tr><th>Model</th><th>Validation</th><th>Test</th></tr></thead><tbody><tr><td>VGG-19</td><td>62.87</td><td>58.33</td></tr><tr><td>SqueezeNet</td><td>68.32</td><td>66.11</td></tr><tr><td>GoogLeNet</td><td>83.00</td><td>81.86</td></tr></tbody></table>"
        },
        "text": "Model Validation Test VGG-19 62.87 58.33 SqueezeNet 68.32 66.11 GoogLeNet 83.00 81.86",
        "type": "Table"
    },
    {
        "element_id": "4696d01133b4ec77a719ae184673b8a3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.4,
                        1284.0
                    ],
                    [
                        101.4,
                        1303.6
                    ],
                    [
                        287.3,
                        1303.6
                    ],
                    [
                        287.3,
                        1284.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.45834,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "3.3. Refine module",
        "type": "Title"
    },
    {
        "element_id": "423aead3b8a103c9228a1584cdeecc36",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        99.9,
                        1341.6
                    ],
                    [
                        99.9,
                        1478.5
                    ],
                    [
                        803.1,
                        1478.5
                    ],
                    [
                        803.1,
                        1341.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93845,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "4696d01133b4ec77a719ae184673b8a3"
        },
        "text": "The refine module connects the image model and the similarity- attention module. The function of this module is to preprocess the feature map extracted by the front network so that the next module can better calculate the mapping relationship between the text and image feature, which will now be of the same dimension.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ffe5714b8fd2601f8f8d003ba8100714",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.0,
                        1485.6
                    ],
                    [
                        104.0,
                        1605.9
                    ],
                    [
                        801.1,
                        1605.9
                    ],
                    [
                        801.1,
                        1485.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93588,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "4696d01133b4ec77a719ae184673b8a3"
        },
        "text": "First, the reshape operation is performed to reduce the high- dimensional feature {y}{., \u00a2RN\u201c\u2122 that is output by the feature extraction network to the low-dimensional feature {vp} \u20ac R\u2122, as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "ed3860e37a1cf776713c9c03be67614b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        117.6,
                        1633.1
                    ],
                    [
                        117.6,
                        1662.6
                    ],
                    [
                        810.1,
                        1662.6
                    ],
                    [
                        810.1,
                        1633.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.65102,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "{mp} = Reshape({v, Moa) (2)",
        "type": "Formula"
    },
    {
        "element_id": "f44a9a0925d121a7b8a51412606fea34",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        99.6,
                        1687.4
                    ],
                    [
                        99.6,
                        1795.2
                    ],
                    [
                        803.1,
                        1795.2
                    ],
                    [
                        803.1,
                        1687.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93621,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "Next, since the reshape operation can obscure part of the feature map\u2019s useful information, a dense layer is added after the reshape layer to adjust the feature map. The sigmoid activation function is used as an activation function of the dense layer, and it can be expressed as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "9f1fcd5fb2cf9e3e0b4d8c5ed46d6312",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        113.6,
                        1816.5
                    ],
                    [
                        113.6,
                        1845.7
                    ],
                    [
                        806.8,
                        1845.7
                    ],
                    [
                        806.8,
                        1816.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.49686,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": ", = sigmoid(v,-W),W \u20ac RY\", 3)",
        "type": "Formula"
    },
    {
        "element_id": "675374e0fa2d0c824d3bb0aedd0571e4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.1,
                        1875.9
                    ],
                    [
                        103.1,
                        1924.1
                    ],
                    [
                        800.0,
                        1924.1
                    ],
                    [
                        800.0,
                        1875.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91098,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "where W denotes the weight matrix of the dense layer, as shown in Fig. 4.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bf10a1965102d7f010549f8be51ee230",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        107.4,
                        1983.5
                    ],
                    [
                        107.4,
                        2004.4
                    ],
                    [
                        454.1,
                        2004.4
                    ],
                    [
                        454.1,
                        1983.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.42912,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "3.4. Embedding and labeling module",
        "type": "Title"
    },
    {
        "element_id": "baf56108083f50c6befb6cbdf2f32369",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        139.6,
                        2041.7
                    ],
                    [
                        139.6,
                        2062.4
                    ],
                    [
                        806.9,
                        2062.4
                    ],
                    [
                        806.9,
                        2041.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83857,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "bf10a1965102d7f010549f8be51ee230"
        },
        "text": "The image model can achieve a comparatively high classification",
        "type": "NarrativeText"
    },
    {
        "element_id": "57924124f6b38c33a42d42d8c153eb78",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1124.4,
                        99.7
                    ],
                    [
                        1124.4,
                        119.2
                    ],
                    [
                        1551.4,
                        119.2
                    ],
                    [
                        1551.4,
                        99.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.5623,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "1087a4287867930b3235869c78b4353b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        865.8,
                        159.4
                    ],
                    [
                        865.8,
                        651.8
                    ],
                    [
                        1543.3,
                        651.8
                    ],
                    [
                        1543.3,
                        159.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92655,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "Output Output f Mat Mul t f Softmax Mat Mul * t Mask(opt.) Softmax 4 \u2018 Scale Similarity * Mat Mul Q K Vv Q K Vv",
        "type": "Image"
    },
    {
        "element_id": "a7c111f71cc76ac66f5151364b4666ae",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.2,
                        672.5
                    ],
                    [
                        853.2,
                        717.6
                    ],
                    [
                        1552.3,
                        717.6
                    ],
                    [
                        1552.3,
                        672.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90725,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "Fig. 5. Similarity-attention process (left) and scaled dot-product attention process (right).",
        "type": "FigureCaption"
    },
    {
        "element_id": "6707714407c7f76fc8f264b91dd6baa4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.0,
                        759.9
                    ],
                    [
                        856.0,
                        868.7
                    ],
                    [
                        1552.3,
                        868.7
                    ],
                    [
                        1552.3,
                        759.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93624,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "accuracy, as shown in Table 1. After the image model, the data are processed by the embedding and labeling module, which performs two operations on the one-hot encoding matrix obtained from the output caption.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a6a0e20a8659f48305f743ee6d9afee9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.9,
                        876.5
                    ],
                    [
                        851.9,
                        1014.8
                    ],
                    [
                        1553.0,
                        1014.8
                    ],
                    [
                        1553.0,
                        876.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9366,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "The first operation is embedding, which compresses the large- dimensional data encoded by the one-hot techniques into a lower- dimensional space. Assume {xo,X1,...,X:} denotes the input sentence, where x; is the ith word in a sentence. Then, the embedding operation can be formulated as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "80f092c8835f1709c9b731cc54eee745",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        857.2,
                        1037.1
                    ],
                    [
                        857.2,
                        1061.6
                    ],
                    [
                        1555.6,
                        1061.6
                    ],
                    [
                        1555.6,
                        1037.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.47208,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "Ey, = We'Xi, (4)",
        "type": "Formula"
    },
    {
        "element_id": "99239b843e90b5de9c21864f2e5d9894",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        850.3,
                        1091.9
                    ],
                    [
                        850.3,
                        1112.1
                    ],
                    [
                        1328.7,
                        1112.1
                    ],
                    [
                        1328.7,
                        1091.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.80418,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "where we denotes the embedding matrix weight.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3aa02a1315543038aa99151979f862ee",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.5,
                        1121.3
                    ],
                    [
                        855.5,
                        1199.9
                    ],
                    [
                        1552.1,
                        1199.9
                    ],
                    [
                        1552.1,
                        1121.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93045,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "The second operation is labeling, which adds the image label infor- mation to the embedded sentence matrix. The final image label can be obtained by the MLC network as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "997e49dbd1a8820632cdca54776405a6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.5,
                        1223.9
                    ],
                    [
                        854.5,
                        1247.9
                    ],
                    [
                        1561.6,
                        1247.9
                    ],
                    [
                        1561.6,
                        1223.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.61974,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "W,, = Add(Ey, sPipred) \u00bb (5)",
        "type": "Formula"
    },
    {
        "element_id": "6994cf95bb2c002fb54a227f66d0f02a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        849.9,
                        1282.0
                    ],
                    [
                        849.9,
                        1301.6
                    ],
                    [
                        1368.4,
                        1301.6
                    ],
                    [
                        1368.4,
                        1282.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84609,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "where Pjpreq denotes the prediction tags distribution.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4d1502429b4292fa4e84eab8faaf3e29",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.0,
                        1347.8
                    ],
                    [
                        854.0,
                        1365.9
                    ],
                    [
                        1156.3,
                        1365.9
                    ],
                    [
                        1156.3,
                        1347.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.75202,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4
        },
        "text": "3.5. Similarity-attention module",
        "type": "Title"
    },
    {
        "element_id": "370cf06136be4426743dcf5c166520a5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.4,
                        1403.9
                    ],
                    [
                        856.4,
                        1511.2
                    ],
                    [
                        1548.3,
                        1511.2
                    ],
                    [
                        1548.3,
                        1403.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93933,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "4d1502429b4292fa4e84eab8faaf3e29"
        },
        "text": "In view of the mission specificity of this study, the similarity- attention module incorporates a similarity-attention mechanism based on the scaled dot-product attention, which represents a fundamental structure of the self-attention mechanism [13], as shown in Fig. 5.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4cccc6621f1335d28dea5f15226270e5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.5,
                        1519.3
                    ],
                    [
                        853.5,
                        1860.2
                    ],
                    [
                        1551.8,
                        1860.2
                    ],
                    [
                        1551.8,
                        1519.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9559,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "4d1502429b4292fa4e84eab8faaf3e29"
        },
        "text": "The similarity-attention module relies on three inputs, namely, query (Q), key (K), and value (V), where Q represents the query matrix and K and V form a key-value pair. In models that use the so-called soft- attention mechanism, it is difficult to learn the relationships between objects correctly in the model training process; hence, such models cannot learn the exact correspondence between image feature and caption information. In contrast, the similarity-attention mechanism\u2019s function is to play a guiding role that directs the focus to the underlying relationship between the feature map and the caption text. In the pro- posed method, the embedded caption data, i.e., the embedding layer\u2019s output, serve as an input to the Q matrix, while K and V are both applied to the refined feature, i.e., the refine module\u2019s output).",
        "type": "NarrativeText"
    },
    {
        "element_id": "953197fbad7acf10f10b8e97c9212405",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.9,
                        1868.8
                    ],
                    [
                        854.9,
                        2062.3
                    ],
                    [
                        1554.8,
                        2062.3
                    ],
                    [
                        1554.8,
                        1868.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9523,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 4,
            "parent_id": "4d1502429b4292fa4e84eab8faaf3e29"
        },
        "text": "The similarity-attention mechanism includes three steps. The first step calculates the similarity value of Q and K, and this value indicates the correspondence between the refined feature and each word in the input sentence. The similarity value is proportional to the correspon- dence between the region of interest and the focal word in the caption text. The similarity matrix between the input sentence and refined feature is defined as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "1cd1f4048901018846d1821b4c852db2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.4,
                        99.6
                    ],
                    [
                        102.4,
                        120.0
                    ],
                    [
                        192.9,
                        120.0
                    ],
                    [
                        192.9,
                        99.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.59213,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "F. Wuet al.",
        "type": "Header"
    },
    {
        "element_id": "69f7afd233c7a5aa11af9747065bfcc7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        119.2,
                        148.5
                    ],
                    [
                        119.2,
                        580.8
                    ],
                    [
                        787.0,
                        580.8
                    ],
                    [
                        787.0,
                        148.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92826,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "asuaq ho WLST \u2014~>F WLS1 yndno ajnpow uojue}3e-AWePWIS asueg 9\u00bb IWLS1 | 0 Exj-1 Ext-1",
        "type": "Image"
    },
    {
        "element_id": "36939ec769c84c8873f8c3cda907045a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.7,
                        604.9
                    ],
                    [
                        101.7,
                        648.1
                    ],
                    [
                        798.4,
                        648.1
                    ],
                    [
                        798.4,
                        604.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9111,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "Fig. 6. The connection between the similarity-attention module and the lan- guage model.",
        "type": "FigureCaption"
    },
    {
        "element_id": "efc336af774557b2bcbfc311c464480a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.7,
                        689.8
                    ],
                    [
                        103.7,
                        747.8
                    ],
                    [
                        811.8,
                        747.8
                    ],
                    [
                        811.8,
                        689.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.775,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "ik iim (Gis) = av (6)",
        "type": "Formula"
    },
    {
        "element_id": "7080a6d37af71aa0ef005a19a63377cd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.8,
                        776.1
                    ],
                    [
                        105.8,
                        855.8
                    ],
                    [
                        799.2,
                        855.8
                    ],
                    [
                        799.2,
                        776.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93286,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "where d* denotes the dimension of K and Q, q; \u20ac Q is ith query, and ke denotes the jth v; \u00a2 V key-value pair. Thus, Eq. (6) defines a function that can be used to calculate similarity scores.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2faf7718375be6820f88ce1a172fc477",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        100.0,
                        801.0
                    ],
                    [
                        100.0,
                        833.0
                    ],
                    [
                        111.0,
                        833.0
                    ],
                    [
                        111.0,
                        801.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "K",
        "type": "Title"
    },
    {
        "element_id": "bffdf9cd4451c34c172b47f7539ce08e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        108.2,
                        865.0
                    ],
                    [
                        108.2,
                        1001.1
                    ],
                    [
                        798.0,
                        1001.1
                    ],
                    [
                        798.0,
                        865.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93895,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5,
            "parent_id": "2faf7718375be6820f88ce1a172fc477"
        },
        "text": "The second step calculates the attention value based on the similarity matrix, as given by Eq. (7). The attention value is set as a soft-max distribution of the similarity value. In the third step, the attention value is multiplied by V to obtain the image feature with the attention region, as given by Eq. (8).",
        "type": "NarrativeText"
    },
    {
        "element_id": "692acf015409159c03289c50bcfdd951",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        100.0,
                        1027.2
                    ],
                    [
                        100.0,
                        1109.3
                    ],
                    [
                        815.5,
                        1109.3
                    ],
                    [
                        815.5,
                        1027.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85585,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "Sreln(o ) (7) i Fam (Gis Kj) =",
        "type": "Formula"
    },
    {
        "element_id": "daaefa11e3fb59c15a884cea02c0e5be",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        118.0,
                        1147.5
                    ],
                    [
                        118.0,
                        1199.8
                    ],
                    [
                        806.5,
                        1199.8
                    ],
                    [
                        806.5,
                        1147.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77656,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "= Do feun (42K) \u00a5) (8) i",
        "type": "Formula"
    },
    {
        "element_id": "a6c1ea5574cb1b56303ceea57dc6982e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        108.0,
                        1151.0
                    ],
                    [
                        108.0,
                        1171.0
                    ],
                    [
                        123.0,
                        1171.0
                    ],
                    [
                        123.0,
                        1151.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "Fe",
        "type": "Title"
    },
    {
        "element_id": "b27c587b9b6988d70877e5081123e745",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.6,
                        1218.0
                    ],
                    [
                        104.6,
                        1502.7
                    ],
                    [
                        805.6,
                        1502.7
                    ],
                    [
                        805.6,
                        1218.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9563,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5,
            "parent_id": "a6c1ea5574cb1b56303ceea57dc6982e"
        },
        "text": "In Eq. (8), \u00a5; denotes the image feature vector with attention value. Similar to the soft-attention model, the attention value of each word in the caption data must be calculated before the data is fed to the LSTM. The adopted approach is to feed the embedded matrix of the entire caption to the attention-similarity module for a one-pass calculation; thus, the attention information is obtained based on the whole input caption, after which the language model can generate words applicable to the overall situation. Similarly, each image feature is fed to the LSTM, which is used to calculate the hidden (initial) state. In this way, the best model is identified by parallel computations.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8d49172c2cd0ae622f9179654dda86c6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.9,
                        1553.6
                    ],
                    [
                        104.9,
                        1572.2
                    ],
                    [
                        307.7,
                        1572.2
                    ],
                    [
                        307.7,
                        1553.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7174,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "3.6. Language model",
        "type": "Title"
    },
    {
        "element_id": "c4b929079ae70d0c6562cb060411bb7b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.0,
                        1610.5
                    ],
                    [
                        104.0,
                        1891.9
                    ],
                    [
                        802.7,
                        1891.9
                    ],
                    [
                        802.7,
                        1610.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95716,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5,
            "parent_id": "8d49172c2cd0ae622f9179654dda86c6"
        },
        "text": "As mentioned previously, the LSTM represents a skeleton of the language model. Unlike how the attention module is connected with a soft-attention mechanism [7], in the language model, the input sen- tence, which is used by the similarity-attention module when calculating the LSTM\u2019s initial states, skips the computation of attention and is instead fed directly to the LSTM input layer after the embedding and labeling module, as shown in Fig. 6. In this way, LSTM\u2019s proximity to the raw data can help the LSTM\u2019s language understanding achieve its full potential. Thus, LSTM yields an exact model of the diagnostic reports by maximizing the joint probability over sentences, as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "0f2d68004dc8e770c7694f0c60c18133",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.8,
                        1914.6
                    ],
                    [
                        104.8,
                        1982.6
                    ],
                    [
                        819.9,
                        1982.6
                    ],
                    [
                        819.9,
                        1914.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84759,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "log p (~ | 91) = So logp(xil xo vst) . (9)",
        "type": "Formula"
    },
    {
        "element_id": "ad4f3ca6aa76c988aaf3a54a2eb67097",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.6,
                        2013.1
                    ],
                    [
                        102.6,
                        2061.8
                    ],
                    [
                        804.5,
                        2061.8
                    ],
                    [
                        804.5,
                        2013.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92266,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "where 6; denotes the LSTM\u2019s internal model parameters, and {xo, x1,... ;Xe} represents the input sentence obtained by the embedding and",
        "type": "NarrativeText"
    },
    {
        "element_id": "dd528002f99223654df4772c787bdd87",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1128.0,
                        100.3
                    ],
                    [
                        1128.0,
                        119.1
                    ],
                    [
                        1553.3,
                        119.1
                    ],
                    [
                        1553.3,
                        100.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71536,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "f2194ab55deac4d30345f81e8e8b0954",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.4,
                        152.2
                    ],
                    [
                        853.4,
                        203.5
                    ],
                    [
                        1549.3,
                        203.5
                    ],
                    [
                        1549.3,
                        152.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92127,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5,
            "parent_id": "dd528002f99223654df4772c787bdd87"
        },
        "text": "labeling module. The LSTM\u2019s initial parameters ho and co are calculated by Eqs. (11) and (12), respectively, where I denotes the input image.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a95b34ce15c30e441811a582b400d97e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        849.5,
                        230.9
                    ],
                    [
                        849.5,
                        278.1
                    ],
                    [
                        1557.3,
                        278.1
                    ],
                    [
                        1557.3,
                        230.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70452,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "(10)",
        "type": "Formula"
    },
    {
        "element_id": "a2a39f45bbbe1bb0d5251a07914b5e79",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.2,
                        302.1
                    ],
                    [
                        855.2,
                        340.4
                    ],
                    [
                        1550.4,
                        340.4
                    ],
                    [
                        1550.4,
                        302.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.56885,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "hy = Wid q1)",
        "type": "Formula"
    },
    {
        "element_id": "663b0b50d0e8e6e03f42b695f09c090e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.5,
                        361.5
                    ],
                    [
                        856.5,
                        393.6
                    ],
                    [
                        1552.9,
                        393.6
                    ],
                    [
                        1552.9,
                        361.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.52731,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "Co = WeP (12)",
        "type": "Formula"
    },
    {
        "element_id": "cc709283b11346e84a1c161e2d52e7ac",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        857.3,
                        411.5
                    ],
                    [
                        857.3,
                        491.8
                    ],
                    [
                        1552.1,
                        491.8
                    ],
                    [
                        1552.1,
                        411.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93243,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "In Eqs. (10)-(12), wp, and w, denote the weight matrices of the dense layer, which lies between the similarity-attention module and the lan- guage model; \u00a5 denotes the similarity-attention module\u2019s output.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b6ea3084ad31f48cab33b10905737151",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.4,
                        499.8
                    ],
                    [
                        855.4,
                        608.5
                    ],
                    [
                        1554.0,
                        608.5
                    ],
                    [
                        1554.0,
                        499.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93572,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "In this process, the LSTM uses the output of word x; from the previous time step and the internal states h;_ and c;_, generated by the preceding LSTM to predict the possible word x; in the next time step, which can be expressed as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "9c635e5f09228bf27565e3cac4fbccf9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.4,
                        631.9
                    ],
                    [
                        853.4,
                        658.3
                    ],
                    [
                        1561.1,
                        658.3
                    ],
                    [
                        1561.1,
                        631.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.60888,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "hy = LSTM(Ex,.,,hi-1,Ci-1) (13)",
        "type": "Formula"
    },
    {
        "element_id": "f80c73245c73cffe2fa54b6f9fcb16f0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        859.7,
                        681.0
                    ],
                    [
                        859.7,
                        712.9
                    ],
                    [
                        1556.2,
                        712.9
                    ],
                    [
                        1556.2,
                        681.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62324,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "P(% | Xoi-1; 8) xexp(hi). (14)",
        "type": "Formula"
    },
    {
        "element_id": "5d1af60081646b83030bc95f910f4954",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        858.7,
                        730.9
                    ],
                    [
                        858.7,
                        809.6
                    ],
                    [
                        1548.9,
                        809.6
                    ],
                    [
                        1548.9,
                        730.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93225,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "Finally, the probability distribution over the dictionary of output words is obtained, and this distribution reflects the dataset and is calculated for the entire network.",
        "type": "NarrativeText"
    },
    {
        "element_id": "49a30a8bc6de3783376bf0e27bd85a2a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        852.9,
                        859.8
                    ],
                    [
                        852.9,
                        879.2
                    ],
                    [
                        1042.6,
                        879.2
                    ],
                    [
                        1042.6,
                        859.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.67252,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "3.7. Model learning",
        "type": "Title"
    },
    {
        "element_id": "c5ee6aa27abc54ebbd99dcd1d4951528",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.5,
                        916.7
                    ],
                    [
                        853.5,
                        1113.9
                    ],
                    [
                        1553.6,
                        1113.9
                    ],
                    [
                        1553.6,
                        916.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94955,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5,
            "parent_id": "49a30a8bc6de3783376bf0e27bd85a2a"
        },
        "text": "Assume {I, l:xX0:i, Xi:1} denote a training data pair, where I is the input image, I, is the corresponding image label, xo., is the whole sen- tence that describes the image I, and x9, \u20ac Xo.n_1 denotes the top i words for arandom integer i less than (n \u2014 1); x;,1 denotes the (i+ 1)th word of the sentence, and it is used to label the language model\u2019s output. Then, the regularized distribution vectors of the image label and the (i+ 1)th word can be respectively defined as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "fb780c38e1605e924582fac4a134383b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        859.9,
                        1132.9
                    ],
                    [
                        859.9,
                        1166.5
                    ],
                    [
                        1557.0,
                        1166.5
                    ],
                    [
                        1557.0,
                        1132.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.6155,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "Pietre = le/ ler, (15)",
        "type": "Formula"
    },
    {
        "element_id": "c6be87ef25c4a992fb382a2907f96a7d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        865.8,
                        1190.8
                    ],
                    [
                        865.8,
                        1227.9
                    ],
                    [
                        1548.7,
                        1227.9
                    ],
                    [
                        1548.7,
                        1190.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62627,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "Prisrarue = Xin / Xipty- (16)",
        "type": "Formula"
    },
    {
        "element_id": "7cfc8ab6294dd94b31dc63ba8bec8787",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        852.7,
                        1241.9
                    ],
                    [
                        852.7,
                        1381.7
                    ],
                    [
                        1553.7,
                        1381.7
                    ],
                    [
                        1553.7,
                        1241.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93398,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "After the training pair is fed to the proposed model, the predicted distribution over all of image labels pj, prea is obtained by the MLC network, and the predicted distribution of the output words Dx, prea is obtained by the LSTM. Then, the image model\u2019s loss function can be formulated as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "ef51a7a29954661424bcce114e77f950",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.8,
                        1404.2
                    ],
                    [
                        855.8,
                        1442.7
                    ],
                    [
                        1555.7,
                        1442.7
                    ],
                    [
                        1555.7,
                        1404.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.68321,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "LF (Pi, prea Phesrues 1591) = \u2014 Yo Phcsrue 108 (Pr. prea) + (17)",
        "type": "Formula"
    },
    {
        "element_id": "bff27551a02bb6329d5990496b7ff565",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        860.5,
                        1468.0
                    ],
                    [
                        860.5,
                        1516.2
                    ],
                    [
                        1545.5,
                        1516.2
                    ],
                    [
                        1545.5,
                        1468.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92202,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "where LF; denotes the loss in the image model, and 6; denotes that model\u2019s parameters.",
        "type": "NarrativeText"
    },
    {
        "element_id": "039d51aec865b9c09e2e79a3255505fd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.8,
                        1526.3
                    ],
                    [
                        854.8,
                        1605.3
                    ],
                    [
                        1552.0,
                        1605.3
                    ],
                    [
                        1552.0,
                        1526.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92887,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "For the rest of the AGNet, the loss function is obtained from the outputs of the refine module, similarity-attention module, embedding and labeling module, and language model, as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "9e5c7062f6f9e1dde83274676f6b2452",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        863.5,
                        1627.0
                    ],
                    [
                        863.5,
                        1662.7
                    ],
                    [
                        1549.1,
                        1662.7
                    ],
                    [
                        1549.1,
                        1627.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.68151,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "LF (Duis pred Passrsruei ts Xois 94) = \u2014 YO Passe 8 (Prsr, pred) > (18)",
        "type": "Formula"
    },
    {
        "element_id": "9d4ac56160788efac64af06c955e6824",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.3,
                        1691.3
                    ],
                    [
                        853.3,
                        1711.6
                    ],
                    [
                        1407.7,
                        1711.6
                    ],
                    [
                        1407.7,
                        1691.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81306,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "where 6, indicates the model parameters of the modules.",
        "type": "NarrativeText"
    },
    {
        "element_id": "397a6d1c344a049a7ae9a136859f858f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.7,
                        1720.7
                    ],
                    [
                        856.7,
                        1770.0
                    ],
                    [
                        1550.2,
                        1770.0
                    ],
                    [
                        1550.2,
                        1720.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88091,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "Overall, the objective of model training is to minimize the ensemble loss, which can be expressed as:",
        "type": "NarrativeText"
    },
    {
        "element_id": "92548cd3f97b94422a672b7f55762153",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        866.5,
                        1792.3
                    ],
                    [
                        866.5,
                        1815.8
                    ],
                    [
                        1550.8,
                        1815.8
                    ],
                    [
                        1550.8,
                        1792.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.58616,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "min{LF(J, le, Xoi,Xis1)} = LF + LF,. (19)",
        "type": "Formula"
    },
    {
        "element_id": "8d24080cdc9fe2042e16b87a6219f798",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.9,
                        1838.7
                    ],
                    [
                        851.9,
                        2062.3
                    ],
                    [
                        1553.1,
                        2062.3
                    ],
                    [
                        1553.1,
                        1838.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95269,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 5
        },
        "text": "The model training process is divided into two training stages: training of the image model and training of the rest of the model. In the second stage, the image model parameters, which are pre-trained in the first stage, are used, and their values are fixed. At each of the two stages, the gradient descent Algorithm is used to update parameters, and Adam\u2019s algorithm [43] is used as an optimizer. The updating process is given in Algorithm 1, in which we use /, and /, to denote f, and f, to the power t.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "59c57442385770785dad68e9de8afece",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.6,
                        99.9
                    ],
                    [
                        102.6,
                        119.5
                    ],
                    [
                        194.1,
                        119.5
                    ],
                    [
                        194.1,
                        99.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.50821,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "F. Wu et al.",
        "type": "Title"
    },
    {
        "element_id": "20b5a0ccbdc41806ae8020e02cd26fef",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1129.6,
                        101.4
                    ],
                    [
                        1129.6,
                        118.7
                    ],
                    [
                        1550.6,
                        118.7
                    ],
                    [
                        1550.6,
                        101.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.77985,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "fe654b7bfa236fae6b7a0edc8efdfd7d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        108.9,
                        153.5
                    ],
                    [
                        108.9,
                        173.5
                    ],
                    [
                        184.3,
                        173.5
                    ],
                    [
                        184.3,
                        153.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.53845,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6,
            "parent_id": "20b5a0ccbdc41806ae8020e02cd26fef"
        },
        "text": "Table 2",
        "type": "Title"
    },
    {
        "element_id": "d22ca16b9af757e951fb0efce8e249b0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        110.4,
                        181.8
                    ],
                    [
                        110.4,
                        200.5
                    ],
                    [
                        294.6,
                        200.5
                    ],
                    [
                        294.6,
                        181.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.27914,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6,
            "parent_id": "fe654b7bfa236fae6b7a0edc8efdfd7d"
        },
        "text": "SPI dataset statistics.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e02750211c20296366021d8b7250a9ec",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        110.2,
                        208.1
                    ],
                    [
                        110.2,
                        518.9
                    ],
                    [
                        795.9,
                        518.9
                    ],
                    [
                        795.9,
                        208.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92375,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6,
            "parent_id": "fe654b7bfa236fae6b7a0edc8efdfd7d",
            "text_as_html": "<table><thead><tr><th>Disease type</th><th>Number of samples</th></tr></thead><tbody><tr><td>Poroma</td><td>24</td></tr><tr><td>Syringoma</td><td>43</td></tr><tr><td>Eczematoid carcinoma</td><td>45</td></tr><tr><td>Lupus miliaris disseminatus faciei</td><td>51</td></tr><tr><td>Neurofibroma</td><td>54</td></tr><tr><td>Pyogenic granuloma</td><td>59</td></tr><tr><td>Dermatofibroma</td><td>113</td></tr><tr><td>Granuloma annulare</td><td>126</td></tr><tr><td>Lichen planus</td><td>134</td></tr><tr><td>Basal cell carcinoma</td><td>224</td></tr><tr><td>Amyloidosis</td><td>274</td></tr></tbody></table>"
        },
        "text": "Disease type Number of samples Poroma 24 Syringoma 43 Eczematoid carcinoma 45 Lupus miliaris disseminatus faciei 51 Neurofibroma 54 Pyogenic granuloma 59 Dermatofibroma 113 Granuloma annulare 126 Lichen planus 134 Basal cell carcinoma 224 Amyloidosis 274",
        "type": "Table"
    },
    {
        "element_id": "85797b8c4364ac180695144a43e8916e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        343.0,
                        1090.9
                    ],
                    [
                        343.0,
                        1391.5
                    ],
                    [
                        690.4,
                        1391.5
                    ],
                    [
                        690.4,
                        1090.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88009,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "40e19a60212dd423ef0016f142fdc991",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        732.2,
                        1037.0
                    ],
                    [
                        732.2,
                        1160.5
                    ],
                    [
                        1302.7,
                        1160.5
                    ],
                    [
                        1302.7,
                        1037.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82897,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "Caption: \u201cHX Al ULAR PEZE AE, Fal Fal 2 22H fd SAAS RAE), YL KAA. \u201d",
        "type": "NarrativeText"
    },
    {
        "element_id": "d05d0b977c9cbca6da32fc618ae70443",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        728.9,
                        1177.9
                    ],
                    [
                        728.9,
                        1334.8
                    ],
                    [
                        1306.8,
                        1334.8
                    ],
                    [
                        1306.8,
                        1177.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89057,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "Denatured fibers can be seen in dermis, and cells are arranged in palisade like arrangement around, and multinucleated giant cells can be seen.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7c4ab22d5e7a782c348f4d2a0949cd01",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        757.0,
                        1339.0
                    ],
                    [
                        757.0,
                        1387.0
                    ],
                    [
                        1085.0,
                        1387.0
                    ],
                    [
                        1085.0,
                        1339.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "Label: \u201cFRA BF Hh\u201d",
        "type": "Title"
    },
    {
        "element_id": "acc74a5403b48cb751a1b9fe95742f12",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        755.2,
                        1389.9
                    ],
                    [
                        755.2,
                        1420.6
                    ],
                    [
                        1057.9,
                        1420.6
                    ],
                    [
                        1057.9,
                        1389.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.30228,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6,
            "parent_id": "7c4ab22d5e7a782c348f4d2a0949cd01"
        },
        "text": "granuloma annulare",
        "type": "NarrativeText"
    },
    {
        "element_id": "9f7d01a2d993adfdc509eb04e04eb1c0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        345.5,
                        1613.1
                    ],
                    [
                        345.5,
                        1908.9
                    ],
                    [
                        686.6,
                        1908.9
                    ],
                    [
                        686.6,
                        1613.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.87354,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "",
        "type": "Image"
    },
    {
        "element_id": "e88fb9ef5b3610230c959c64fc2d85f0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        716.6,
                        1499.6
                    ],
                    [
                        716.6,
                        1677.6
                    ],
                    [
                        1313.1,
                        1677.6
                    ],
                    [
                        1313.1,
                        1499.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92951,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "Caption: \u201c4X Ae, FAA 4, Wea. AA WA Be RAM, FEM, FREER EB ll YC Ai PLR.\u201d",
        "type": "NarrativeText"
    },
    {
        "element_id": "279e8624de14b66cc0a4be84d6c0f6b5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        718.1,
                        1684.6
                    ],
                    [
                        718.1,
                        1926.7
                    ],
                    [
                        1310.9,
                        1926.7
                    ],
                    [
                        1310.9,
                        1684.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93849,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "Epidermis hyperkeratosis, incomplete keratosis, scab visible. A large number of blood vessels and inflammatory cells were found in dermis, and small abscesses were formed. The basal epithelium contracted inward into a ring.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d016eb815a9cff01390d4b1034b9fc15",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        747.8,
                        1930.7
                    ],
                    [
                        747.8,
                        1972.9
                    ],
                    [
                        1115.0,
                        1972.9
                    ],
                    [
                        1115.0,
                        1930.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70999,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "Label: \u201c40/4 VE 3 ih\u201d",
        "type": "NarrativeText"
    },
    {
        "element_id": "265d05659f86622b1f2d06631892ad87",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        744.0,
                        1980.1
                    ],
                    [
                        744.0,
                        2015.0
                    ],
                    [
                        1054.2,
                        2015.0
                    ],
                    [
                        1054.2,
                        1980.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.42809,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "pyogenic granuloma",
        "type": "Title"
    },
    {
        "element_id": "a417ce6ec98ccfa3057ba8fe50e104a1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        573.3,
                        2040.6
                    ],
                    [
                        573.3,
                        2063.0
                    ],
                    [
                        1076.2,
                        2063.0
                    ],
                    [
                        1076.2,
                        2040.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.59158,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6,
            "parent_id": "265d05659f86622b1f2d06631892ad87"
        },
        "text": "Fig. 7. Examples of training data from the SPI dataset.",
        "type": "NarrativeText"
    },
    {
        "element_id": "cff234f5732079bb3984637d367fe5c3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        820.9,
                        2099.6
                    ],
                    [
                        820.9,
                        2121.1
                    ],
                    [
                        833.6,
                        2121.1
                    ],
                    [
                        833.6,
                        2099.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.62723,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 6
        },
        "text": "6",
        "type": "Footer"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "b45aa166cf4a83da04b0c40c5de9b7c5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.3,
                        99.7
                    ],
                    [
                        103.3,
                        120.5
                    ],
                    [
                        195.9,
                        120.5
                    ],
                    [
                        195.9,
                        99.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.73095,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7
        },
        "text": "F. Wu et al.",
        "type": "Header"
    },
    {
        "element_id": "8580e853c9992f7d10b77d9bb542827f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.3,
                        150.8
                    ],
                    [
                        102.3,
                        173.4
                    ],
                    [
                        240.7,
                        173.4
                    ],
                    [
                        240.7,
                        150.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.56168,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "b45aa166cf4a83da04b0c40c5de9b7c5"
        },
        "text": "Algorithm 1.",
        "type": "Title"
    },
    {
        "element_id": "4d78b5c7a5df053bc45bebb15894eb2c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        178.9,
                        202.9
                    ],
                    [
                        178.9,
                        225.1
                    ],
                    [
                        310.4,
                        225.1
                    ],
                    [
                        310.4,
                        202.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.36089,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "b45aa166cf4a83da04b0c40c5de9b7c5"
        },
        "text": "Algorithm 1",
        "type": "Title"
    },
    {
        "element_id": "057632e903c6fffc3e9e507fbb15134e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        152.5,
                        244.2
                    ],
                    [
                        152.5,
                        271.6
                    ],
                    [
                        492.6,
                        271.6
                    ],
                    [
                        492.6,
                        244.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.59596,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "4d78b5c7a5df053bc45bebb15894eb2c"
        },
        "text": "Require: a \u20ac [0,1]: learning rate",
        "type": "NarrativeText"
    },
    {
        "element_id": "f97b27933b67bf0092b47cd0c4756b71",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        155.2,
                        290.5
                    ],
                    [
                        155.2,
                        313.5
                    ],
                    [
                        709.2,
                        313.5
                    ],
                    [
                        709.2,
                        290.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.54528,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "4d78b5c7a5df053bc45bebb15894eb2c"
        },
        "text": "Require: f, Bz \u20ac [0,1): exponential decay rates of the",
        "type": "NarrativeText"
    },
    {
        "element_id": "e70a49ef7409f3e6841ac732d649954e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        153.3,
                        335.5
                    ],
                    [
                        153.3,
                        356.4
                    ],
                    [
                        334.4,
                        356.4
                    ],
                    [
                        334.4,
                        335.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.57866,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "4d78b5c7a5df053bc45bebb15894eb2c"
        },
        "text": "moment estimates",
        "type": "NarrativeText"
    },
    {
        "element_id": "0a0c4c52ee7713fe1279b37e1270d8e1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        153.5,
                        376.5
                    ],
                    [
                        153.5,
                        399.4
                    ],
                    [
                        650.0,
                        399.4
                    ],
                    [
                        650.0,
                        376.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.75072,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "4d78b5c7a5df053bc45bebb15894eb2c"
        },
        "text": "Require: f (@): stochastic objective function with",
        "type": "NarrativeText"
    },
    {
        "element_id": "9a70dfdc8d1ad4a8cc9810d873ddeaeb",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        153.8,
                        421.4
                    ],
                    [
                        153.8,
                        442.9
                    ],
                    [
                        283.0,
                        442.9
                    ],
                    [
                        283.0,
                        421.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.54482,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "4d78b5c7a5df053bc45bebb15894eb2c"
        },
        "text": "parameters 6",
        "type": "NarrativeText"
    },
    {
        "element_id": "de1657d284d84ffe1854a7a28ca24679",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        155.9,
                        462.0
                    ],
                    [
                        155.9,
                        487.1
                    ],
                    [
                        524.9,
                        487.1
                    ],
                    [
                        524.9,
                        462.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7632,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "4d78b5c7a5df053bc45bebb15894eb2c"
        },
        "text": "Require: 9 : initial parameter vector",
        "type": "NarrativeText"
    },
    {
        "element_id": "33837a040d3848c73dc34b94b1c05d72",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        153.8,
                        509.6
                    ],
                    [
                        153.8,
                        529.3
                    ],
                    [
                        255.4,
                        529.3
                    ],
                    [
                        255.4,
                        509.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.44238,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "b45aa166cf4a83da04b0c40c5de9b7c5"
        },
        "text": "Initialize:",
        "type": "Title"
    },
    {
        "element_id": "9c3666ba69bd8d479c32ca57043be9c8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        162.8,
                        551.6
                    ],
                    [
                        162.8,
                        572.7
                    ],
                    [
                        248.6,
                        572.7
                    ],
                    [
                        248.6,
                        551.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.3221,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "33837a040d3848c73dc34b94b1c05d72"
        },
        "text": "My, \u20140",
        "type": "NarrativeText"
    },
    {
        "element_id": "851dcc19b0ed36941992d2a693764471",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        163.7,
                        593.3
                    ],
                    [
                        163.7,
                        615.1
                    ],
                    [
                        240.9,
                        615.1
                    ],
                    [
                        240.9,
                        593.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.39804,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "33837a040d3848c73dc34b94b1c05d72"
        },
        "text": "<0",
        "type": "NarrativeText"
    },
    {
        "element_id": "b0ed1124cd6cfd82ae3c994f38b5d878",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        167.0,
                        638.0
                    ],
                    [
                        167.0,
                        655.0
                    ],
                    [
                        223.0,
                        655.0
                    ],
                    [
                        223.0,
                        638.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "33837a040d3848c73dc34b94b1c05d72"
        },
        "text": "t<-0",
        "type": "UncategorizedText"
    },
    {
        "element_id": "2ce5f6ddf6c4ec64b94de1120f125541",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        167.3,
                        676.9
                    ],
                    [
                        167.3,
                        702.0
                    ],
                    [
                        262.7,
                        702.0
                    ],
                    [
                        262.7,
                        676.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.29219,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "33837a040d3848c73dc34b94b1c05d72"
        },
        "text": "e< 10%",
        "type": "NarrativeText"
    },
    {
        "element_id": "db2f7a434378c3d81ad407888b225365",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        154.6,
                        723.9
                    ],
                    [
                        154.6,
                        746.3
                    ],
                    [
                        265.6,
                        746.3
                    ],
                    [
                        265.6,
                        723.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.54363,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "b45aa166cf4a83da04b0c40c5de9b7c5"
        },
        "text": "Calculate:",
        "type": "Title"
    },
    {
        "element_id": "235ff55cf992b61c374a94fac93c4993",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        165.2,
                        765.7
                    ],
                    [
                        165.2,
                        789.5
                    ],
                    [
                        484.3,
                        789.5
                    ],
                    [
                        484.3,
                        765.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78574,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "db2f7a434378c3d81ad407888b225365"
        },
        "text": "while 6, has not converged, do:",
        "type": "NarrativeText"
    },
    {
        "element_id": "fecbe33433a1464d9e3fd497b1c9b03b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        176.1,
                        810.2
                    ],
                    [
                        176.1,
                        832.0
                    ],
                    [
                        277.2,
                        832.0
                    ],
                    [
                        277.2,
                        810.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.38484,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "db2f7a434378c3d81ad407888b225365"
        },
        "text": "tettl1",
        "type": "NarrativeText"
    },
    {
        "element_id": "cba314036c4d94721c78cddaf50504b2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        173.3,
                        839.0
                    ],
                    [
                        173.3,
                        1154.7
                    ],
                    [
                        506.5,
                        1154.7
                    ],
                    [
                        506.5,
                        839.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.49383,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "db2f7a434378c3d81ad407888b225365"
        },
        "text": "Ie \u2014 Vofi(-1) m, \u2014 By Me-1 + (1 \u2014 Bi) * Ge V_ & Bo V_4 + (1\u2014 Bo) Gg? Mm, \u2014m, / (1\u2014 Bt) bv, / (1-63) 0, \u2014 O;-1 -\u2014 a, / (8; + \u20ac) end",
        "type": "NarrativeText"
    },
    {
        "element_id": "2150594c3984e6b72b04b29d3630078e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        164.5,
                        1165.9
                    ],
                    [
                        164.5,
                        1186.3
                    ],
                    [
                        260.6,
                        1186.3
                    ],
                    [
                        260.6,
                        1165.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48385,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "db2f7a434378c3d81ad407888b225365"
        },
        "text": "return 6,",
        "type": "NarrativeText"
    },
    {
        "element_id": "aaf1b7e4e7a8760fc07c654dbd43aa7c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.7,
                        1281.9
                    ],
                    [
                        103.7,
                        1301.8
                    ],
                    [
                        306.5,
                        1301.8
                    ],
                    [
                        306.5,
                        1281.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48794,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "db2f7a434378c3d81ad407888b225365"
        },
        "text": "3.8. Model inference",
        "type": "ListItem"
    },
    {
        "element_id": "94d30c4fb632dcb2e19f5026f1e9bab2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        109.5,
                        1339.2
                    ],
                    [
                        109.5,
                        1507.1
                    ],
                    [
                        807.8,
                        1507.1
                    ],
                    [
                        807.8,
                        1339.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9552,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "db2f7a434378c3d81ad407888b225365"
        },
        "text": "Unlike the model training, for model inference, the initial data pair is {I,xo}, where Xp is the {START} signal, which is treated as a word to be added to the sentence. The Algorithm\u2019s cyclic structure is designed such that, at each time step, the input data pair is added to the output word of the previous time step. After the one-hot decoding, if the LSTM\u2019s output value is a {STOP} signal, then the cycle is terminated.",
        "type": "NarrativeText"
    },
    {
        "element_id": "51403d4b61dfae34cd3af7d7f001e82b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.8,
                        1549.0
                    ],
                    [
                        101.8,
                        1568.7
                    ],
                    [
                        361.7,
                        1568.7
                    ],
                    [
                        361.7,
                        1549.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83765,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "b45aa166cf4a83da04b0c40c5de9b7c5"
        },
        "text": "4. Experimental results",
        "type": "Title"
    },
    {
        "element_id": "ab139c2c98cd4cb3ed1db4de968cd0bb",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        107.5,
                        1606.8
                    ],
                    [
                        107.5,
                        1626.2
                    ],
                    [
                        469.5,
                        1626.2
                    ],
                    [
                        469.5,
                        1606.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.41521,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "b45aa166cf4a83da04b0c40c5de9b7c5"
        },
        "text": "4.1. Data collection and preprocessing",
        "type": "Title"
    },
    {
        "element_id": "57c429cfe6eda801780a52334ab69613",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        99.5,
                        1663.9
                    ],
                    [
                        99.5,
                        2062.2
                    ],
                    [
                        805.1,
                        2062.2
                    ],
                    [
                        805.1,
                        1663.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95461,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "ab139c2c98cd4cb3ed1db4de968cd0bb"
        },
        "text": "For model training and evaluation, a new skin pathological image (SPI) dataset consisting of 1147 pieces of data was constructed. Each data sample included an image of skin pathology, the disease name, and the corresponding diagnosis report. The data were collected by a multitude of dermatologists during their one-decade diagnosis practice. The data were all subjected to the second-validity check to ensure ac- curacy. The dataset statistics are given in Table 2, where 11 disease types are given, including Lichen planus, Syringoma, Poroma, Pyogenic granuloma, Granuloma annulare, Basal cell carcinoma, Amyloidosis, Dermatofibroma, Neurofibroma, Eczematoid carcinoma, and Lupus miliaris disseminatus faciei. In this work, the disease name was considered a pathological image\u2019s label, and the diagnosis report was considered a caption. All non-alphabetic tokens and participles were removed by preprocessing the caption data. Considering a rigid format",
        "type": "NarrativeText"
    },
    {
        "element_id": "6aaca3dc6b0672d04918fcf5a9a27ca3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1129.1,
                        101.0
                    ],
                    [
                        1129.1,
                        119.0
                    ],
                    [
                        1553.5,
                        119.0
                    ],
                    [
                        1553.5,
                        101.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.63772,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "b3eab46c9e4160831d103a026551d3b9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.4,
                        152.6
                    ],
                    [
                        854.4,
                        404.8
                    ],
                    [
                        1553.8,
                        404.8
                    ],
                    [
                        1553.8,
                        152.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95627,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "6aaca3dc6b0672d04918fcf5a9a27ca3"
        },
        "text": "of the caption data and the correlations between individual words, an expression dictionary was constructed on the basis of segmented caption data. The coding length of the caption data and the number of param- eters were reduced, and this process eventually yielded the normalized caption data suitable for model training. Each caption contained 25 segmented words on average, and the dictionary contained a total of 595 words. The data were randomly divided into training, validation, and test sets accounting for 70%, 15%, and 15% of the overall data, respectively.",
        "type": "NarrativeText"
    },
    {
        "element_id": "9680fb08964995c24816b0b16aa24d93",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.4,
                        444.4
                    ],
                    [
                        854.4,
                        463.5
                    ],
                    [
                        1109.4,
                        463.5
                    ],
                    [
                        1109.4,
                        444.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78992,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "6aaca3dc6b0672d04918fcf5a9a27ca3"
        },
        "text": "4.2. Model implementation",
        "type": "Title"
    },
    {
        "element_id": "5f0dcda37a56e71f9faa1958bc05680f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.4,
                        500.4
                    ],
                    [
                        856.4,
                        697.3
                    ],
                    [
                        1553.7,
                        697.3
                    ],
                    [
                        1553.7,
                        500.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95197,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "9680fb08964995c24816b0b16aa24d93"
        },
        "text": "The model training process was divided into two steps, and the ex- amples of the training data are shown in Fig. 7. The first step was to pretrain the image model on the SPI dataset using images and the cor- responding image label information. Several CNN-based image classifi- cation models were compared, including the VGG-19 [44], SqueezeNet [45], and GoogLeNet (Inception v3) [42], to evaluate the quality and reliability of learned image features.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f507430ed9f091a61720e659f09baa2c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.9,
                        704.5
                    ],
                    [
                        854.9,
                        898.4
                    ],
                    [
                        1554.8,
                        898.4
                    ],
                    [
                        1554.8,
                        704.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94794,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "9680fb08964995c24816b0b16aa24d93"
        },
        "text": "Among all the tested classifiers, GoogLeNet exhibited the highest accuracy on the dataset, as shown in Table 1; therefore, the integrated GoogLeNet was selected as an image model. Next, the weight parameters of the image model\u2019s hidden layer were extracted to obtain the correct image features, whose relevance was guaranteed by high classification accuracy. Next, the pretraining parameters of the trained image model were optimized.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a61f64fc36e0647ba15443c71e2ad5dc",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.2,
                        907.6
                    ],
                    [
                        855.2,
                        1044.8
                    ],
                    [
                        1552.0,
                        1044.8
                    ],
                    [
                        1552.0,
                        907.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94264,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "9680fb08964995c24816b0b16aa24d93"
        },
        "text": "The second training step was to train the LSTM, similarity-attention module, and modules that followed the image model simultaneously. In this step, the weight parameters of the image model were also fixed. In the text generation task, the number of hidden states and embedded dimensions was set to 512.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6f67af9fd95ff7ab09e345f5dcf727ff",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.2,
                        1052.6
                    ],
                    [
                        856.2,
                        1277.6
                    ],
                    [
                        1555.4,
                        1277.6
                    ],
                    [
                        1555.4,
                        1052.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95281,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "9680fb08964995c24816b0b16aa24d93"
        },
        "text": "During the caption data processing, the sentences were appended by adding {START} at the head of the caption and {STOP} at the tail of the caption. After that, one-hot coding was performed in accordance with the expression dictionary obtained from the SPI dataset. In both model training steps, the multi-classification cross-entropy loss function and the Adam optimizer [43] were used, adopting the learning rates of dy- namic attenuation of 0.01 and 0.001, respectively. The learning rate\u2019s update process was as follows:",
        "type": "NarrativeText"
    },
    {
        "element_id": "6b1d1aa819c61babcd161800dab3abe8",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.6,
                        1299.1
                    ],
                    [
                        853.6,
                        1329.3
                    ],
                    [
                        1556.0,
                        1329.3
                    ],
                    [
                        1556.0,
                        1299.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.54592,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7
        },
        "text": "LRELR-yf, (20)",
        "type": "Formula"
    },
    {
        "element_id": "66fe4d6ffe535aff2f531800dd3c94ae",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.3,
                        1355.0
                    ],
                    [
                        855.3,
                        1405.2
                    ],
                    [
                        1555.8,
                        1405.2
                    ],
                    [
                        1555.8,
                        1355.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92655,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7
        },
        "text": "where LR stands for the learning rate, 1 denotes the decay rate, and \u00a2 is the control signal for the training step.",
        "type": "NarrativeText"
    },
    {
        "element_id": "cafc7977011f4932337e3a63c45c9978",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.7,
                        1413.1
                    ],
                    [
                        853.7,
                        1578.4
                    ],
                    [
                        1553.7,
                        1578.4
                    ],
                    [
                        1553.7,
                        1413.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95126,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7
        },
        "text": "Each training batch contained 36 images and their corresponding caption data. During the training process, the best model-saving strategy was determined based on the loss function value calculated using the verification data. To prevent overfitting, the process invoked the early- stop operation; namely, when the validation data\u2019s accuracy exceeded five epochs, the model automatically stopped the training process.",
        "type": "NarrativeText"
    },
    {
        "element_id": "8420772bd72d9a708b5e07392a9009d3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        852.6,
                        1616.1
                    ],
                    [
                        852.6,
                        1635.5
                    ],
                    [
                        987.8,
                        1635.5
                    ],
                    [
                        987.8,
                        1616.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.69398,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7
        },
        "text": "4.3. Baselines",
        "type": "Title"
    },
    {
        "element_id": "0066f4648bdf42fa6991c71d9c5c6fcd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.8,
                        1673.6
                    ],
                    [
                        853.8,
                        1985.4
                    ],
                    [
                        1554.5,
                        1985.4
                    ],
                    [
                        1554.5,
                        1673.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9556,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 7,
            "parent_id": "8420772bd72d9a708b5e07392a9009d3"
        },
        "text": "The AGNet was compared with two image caption models, the CNN- RNN model [46], and the soft-attention model [7], while using the pre-trained Inception v3 as a CNN backbone, and three newer image caption models, A2I2 [47], BUTD [12], and AdaAtt [9]. To evaluate the advantages of the proposed similarity-attention module, a no-similarity model was implemented by removing the similarity calculation from the attention module while retaining the two following dense layers in that module. Similarly, the labeling operation\u2019s effect on the model text generation was analyzed by implementing a no-labeling model with the original AGNet; in this model, the embedding and labeling module was reduced to the embedding operation only.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "ae504ab25067e171319bd9e272cf7c92",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.0,
                        99.3
                    ],
                    [
                        102.0,
                        120.9
                    ],
                    [
                        194.2,
                        120.9
                    ],
                    [
                        194.2,
                        99.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74435,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8
        },
        "text": "F. Wu et al.",
        "type": "Header"
    },
    {
        "element_id": "4a439e3fff69a8dafca94da370db8e28",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1129.9,
                        101.4
                    ],
                    [
                        1129.9,
                        118.8
                    ],
                    [
                        1551.9,
                        118.8
                    ],
                    [
                        1551.9,
                        101.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82654,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "fdb0d27a9cc997a930cb5dfce74a57c2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.5,
                        153.7
                    ],
                    [
                        105.5,
                        173.7
                    ],
                    [
                        175.6,
                        173.7
                    ],
                    [
                        175.6,
                        153.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.66294,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "4a439e3fff69a8dafca94da370db8e28"
        },
        "text": "Table 3",
        "type": "Title"
    },
    {
        "element_id": "cf53e476d93ffe4fa64c3ef5c4d89de5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.1,
                        180.9
                    ],
                    [
                        101.1,
                        201.0
                    ],
                    [
                        590.1,
                        201.0
                    ],
                    [
                        590.1,
                        180.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.65565,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "fdb0d27a9cc997a930cb5dfce74a57c2"
        },
        "text": "Accuracy scores of different models on the SPI dataset.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e7c2bd0ace9fe65e19714de5ea7f76c5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        97.0,
                        203.2
                    ],
                    [
                        97.0,
                        447.7
                    ],
                    [
                        1547.7,
                        447.7
                    ],
                    [
                        1547.7,
                        203.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92288,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "fdb0d27a9cc997a930cb5dfce74a57c2",
            "text_as_html": "<table><thead><tr><th>Model</th><th>Dataset</th><th>Labeling</th><th>Similarity</th><th>Bl</th><th>B2</th><th>B3</th><th>B4</th><th>M</th><th>R</th></tr></thead><tbody><tr><td>A212</td><td>Full-SPI</td><td></td><td></td><td>10.40</td><td>4.80</td><td>2.60</td><td>1.80</td><td>9.70</td><td>16.70</td></tr><tr><td>BUTD</td><td>Full-SPI</td><td></td><td></td><td>10.50</td><td>5.00</td><td>2.70</td><td>1.90</td><td>10.20</td><td>17.30</td></tr><tr><td>AdaAtt</td><td>Full-SPI</td><td></td><td></td><td>10.40</td><td>5.00</td><td>2.80</td><td>1.90</td><td>10.30</td><td>17.20</td></tr><tr><td>CNN-RNN-</td><td>Full-SPI</td><td></td><td></td><td>48.88</td><td>40.67</td><td>37.79</td><td>37.23</td><td>49.37</td><td>48.67</td></tr><tr><td>SAT</td><td>Full-SPI</td><td></td><td></td><td>74.27</td><td>70.47</td><td>68.65,</td><td>67.54</td><td>76.88</td><td>76.76</td></tr><tr><td>AGNet</td><td>Full-SPI</td><td>v</td><td></td><td>74.37</td><td>70.84</td><td>69.28</td><td>68.37</td><td>76.45</td><td>77.31</td></tr><tr><td>AGNet</td><td>Full-SPI</td><td></td><td>v</td><td>74.41</td><td>70.96</td><td>69.47</td><td>68.65</td><td>76.34</td><td>77.12</td></tr><tr><td>AGNet</td><td>Full-SPI</td><td>v</td><td>v</td><td>75.75</td><td>72.02</td><td>70.32</td><td>69.34</td><td>77.52</td><td>78.14</td></tr></tbody></table>"
        },
        "text": "Model Dataset Labeling Similarity Bl B2 B3 B4 M R A212 Full-SPI 10.40 4.80 2.60 1.80 9.70 16.70 BUTD Full-SPI 10.50 5.00 2.70 1.90 10.20 17.30 AdaAtt Full-SPI 10.40 5.00 2.80 1.90 10.30 17.20 CNN-RNN Full-SPI 48.88 40.67 37.79 37.23 49.37 48.67 SAT Full-SPI 74.27 70.47 68.65, 67.54 76.88 76.76 AGNet Full-SPI v 74.37 70.84 69.28 68.37 76.45 77.31 AGNet Full-SPI v 74.41 70.96 69.47 68.65 76.34 77.12 AGNet Full-SPI v v 75.75 72.02 70.32 69.34 77.52 78.14",
        "type": "Table"
    },
    {
        "element_id": "377b162cc79ed89521619e14dd2cb131",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.5,
                        458.8
                    ],
                    [
                        105.5,
                        480.1
                    ],
                    [
                        1254.9,
                        480.1
                    ],
                    [
                        1254.9,
                        458.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81368,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "fdb0d27a9cc997a930cb5dfce74a57c2"
        },
        "text": "Note: B1-B4 = BLEU1-BLEU4; M = METEOR; R = ROUGE; SAT = soft attention model. Highest values are denoted in bold italic.",
        "type": "NarrativeText"
    },
    {
        "element_id": "54e263ba6112a5cf66201cec49f7a6b0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.9,
                        538.6
                    ],
                    [
                        102.9,
                        558.3
                    ],
                    [
                        176.6,
                        558.3
                    ],
                    [
                        176.6,
                        538.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.72848,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "4a439e3fff69a8dafca94da370db8e28"
        },
        "text": "Table 4",
        "type": "Title"
    },
    {
        "element_id": "aac191736478a90d13c79524127606ed",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.9,
                        566.4
                    ],
                    [
                        104.9,
                        585.6
                    ],
                    [
                        613.8,
                        585.6
                    ],
                    [
                        613.8,
                        566.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.48957,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "54e263ba6112a5cf66201cec49f7a6b0"
        },
        "text": "Accuracy scores of different models on the three datasets.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d1a7aceab2410859fe90a0b2107e94e4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.7,
                        592.2
                    ],
                    [
                        102.7,
                        891.1
                    ],
                    [
                        1552.0,
                        891.1
                    ],
                    [
                        1552.0,
                        592.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93211,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "54e263ba6112a5cf66201cec49f7a6b0",
            "text_as_html": "<table><thead><tr><th rowspan=\"2\">Model</th><th colspan=\"3\">Sample size</th><th rowspan=\"2\">Bl</th><th rowspan=\"2\">B2</th><th rowspan=\"2\">B3</th><th rowspan=\"2\">B4</th><th rowspan=\"2\">M</th><th rowspan=\"2\">R</th></tr><tr><th>Small</th><th>Medium</th><th>Large</th></tr></thead><tbody><tr><td>CNN-RNN-</td><td>v</td><td></td><td></td><td>22.13</td><td>7.25,</td><td>1.21</td><td></td><td>19.47</td><td>20.50</td></tr><tr><td>SAT</td><td>v</td><td></td><td></td><td>43.34</td><td>33.33</td><td>29.17</td><td>28.09</td><td>45.74</td><td>45.80</td></tr><tr><td>AGNet</td><td>v</td><td></td><td></td><td>45.35</td><td>35.91</td><td>32.38</td><td>30.71</td><td>47.30</td><td>46.21</td></tr><tr><td>CNN-RNN-</td><td></td><td>v</td><td></td><td>17.05</td><td>4.63</td><td></td><td></td><td>13.02</td><td>15.50</td></tr><tr><td>SAT</td><td></td><td>v</td><td></td><td>75.61</td><td>71.83</td><td>70.62</td><td>70.01</td><td>76.52</td><td>77.62</td></tr><tr><td>AGNet</td><td></td><td>v</td><td></td><td>76.27</td><td>71.91</td><td>69.70</td><td>68.45</td><td>78.37</td><td>77.26</td></tr><tr><td>CNN-RNN-</td><td></td><td></td><td>v</td><td>85.08</td><td>83.36</td><td>82.51</td><td>81.85</td><td>87.92</td><td>87.85</td></tr><tr><td>SAT</td><td></td><td></td><td>v</td><td>80.38</td><td>77.93</td><td>76.53</td><td>75.48</td><td>84.85</td><td>83.27</td></tr><tr><td>AGNet</td><td></td><td></td><td>v</td><td>87.44</td><td>85.99</td><td>85.20</td><td>84.55</td><td>90.17</td><td>90.16</td></tr></tbody></table>"
        },
        "text": "Model Sample size Bl B2 B3 B4 M R Small Medium Large CNN-RNN v 22.13 7.25 1.21 - 19.47 20.50 SAT v 43.34 33.33 29.17 28.09 45.74 45.80 AGNet v 45.35 35.91 32.38 30.71 47.30 46.21 CNN-RNN v 17.05 4.63 - - 13.02 15.50 SAT v 75.61 71.83 70.62 70.01 76.52 77.62 AGNet v 76.27 71.91 69.70 68.45 78.37 77.26 CNN-RNN v 85.08 83.36 82.51 81.85 87.92 87.85 SAT v 80.38 77.93 76.53 75.48 84.85 83.27 AGNet v 87.44 85.99 85.20 84.55 90.17 90.16",
        "type": "Table"
    },
    {
        "element_id": "17ccfab34585743354f622601a51edf7",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.0,
                        904.3
                    ],
                    [
                        104.0,
                        924.9
                    ],
                    [
                        1254.9,
                        924.9
                    ],
                    [
                        1254.9,
                        904.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.82034,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "54e263ba6112a5cf66201cec49f7a6b0"
        },
        "text": "Note: B1-B4 = BLEU1-BLEU4; M = METEOR; R = ROUGE; SAT = soft attention model. Highest values are denoted in bold italic.",
        "type": "NarrativeText"
    },
    {
        "element_id": "87dbcef1c2e1c62d2b597df11058eb4a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.0,
                        965.8
                    ],
                    [
                        104.0,
                        986.8
                    ],
                    [
                        336.9,
                        986.8
                    ],
                    [
                        336.9,
                        965.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.6103,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "4a439e3fff69a8dafca94da370db8e28"
        },
        "text": "4.4. Quantitative results",
        "type": "Title"
    },
    {
        "element_id": "e8da2055eb17f5e59e950aaf7a5d2d5a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.8,
                        1024.1
                    ],
                    [
                        101.8,
                        1423.0
                    ],
                    [
                        805.2,
                        1423.0
                    ],
                    [
                        805.2,
                        1024.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95573,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "87dbcef1c2e1c62d2b597df11058eb4a"
        },
        "text": "The proposed model performance was evaluated in terms of several standard caption evaluation measures, including BLEU1-BLEU4 [48], METEOR [49], and ROUGE [50]. Due to the imbalance between different data categories in the SPI dataset, neither the AGNet model nor baseline models achieved significantly better performances than the other models on data categories including large data samples. However, the models\u2019 scores differed significantly on data categories with small samples. Therefore, the dataset was divided into four segments: (i) the complete dataset; (ii) a large dataset, which included categories with large samples; (iii) a medium dataset, which included categories with medium samples; and (iv) a small dataset, which included categories with small samples. As shown in Table 3, the AGNet model performed better than the baseline models on the complete SPI dataset in terms of generating captions.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5d20aca1bd0897c4f9e93f6cd3f28f58",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.1,
                        1430.8
                    ],
                    [
                        103.1,
                        1683.2
                    ],
                    [
                        805.3,
                        1683.2
                    ],
                    [
                        805.3,
                        1430.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95507,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "87dbcef1c2e1c62d2b597df11058eb4a"
        },
        "text": "For the CNN-RNN model, which did not apply an attention mecha- nism when processing the image or generating a caption, the perfor- mance was understandably worse than those of the models that included the attention mechanism. The effectiveness of AGNet\u2019s internal modules was verified by experiments. As shown in Table 3, and as expected, canceling either the labeling module or the similarity-calculation mod- ule from the proposed model reduced the proposed model\u2019s perfor- mance compared with its complete version. These results indicate that the embedding and labeling module and the similarity-attention module",
        "type": "NarrativeText"
    },
    {
        "element_id": "8b57710463470addf2cd3fcf29538b59",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.8,
                        966.7
                    ],
                    [
                        855.8,
                        1044.1
                    ],
                    [
                        1550.8,
                        1044.1
                    ],
                    [
                        1550.8,
                        966.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93393,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "87dbcef1c2e1c62d2b597df11058eb4a"
        },
        "text": "help the LSTM understand the caption data better. Moreover, as shown in Table 4, the AGNet model outperformed the other models on datasets of various sizes, indicating good robustness of the proposed model.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6b5a021578c89af65ae2b3b6252cebe9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.2,
                        1049.6
                    ],
                    [
                        855.2,
                        1539.6
                    ],
                    [
                        1556.2,
                        1539.6
                    ],
                    [
                        1556.2,
                        1049.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95333,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "87dbcef1c2e1c62d2b597df11058eb4a"
        },
        "text": "In addition to evaluating the proposed model on a testing set, the variation in the loss function value during the model training process was also analyzed. The result showed that the soft-attention model had difficulty in identifying the correct gradient descent, and the LSTM performed poorly in learning from caption data. Thus, this model re- quires more training epochs to achieve convergence. In addition, the results of several hyper-parameter fine-tuning tests showed that the soft- attention model could seldom accelerate model fitting by modifying the learning rate. To isolate the effect of inferences due to Adam\u2019s optimizer, the soft-attention model\u2019s results were compared for the RMSProp optimizer [51] and Adam\u2019s optimizer. In the latter case, it still took more than 200 epochs to achieve convergence. The suboptimal baseline re- sults indicated that the AGNet with fewer operations could feed the caption data directly to the LSTM and then separate the attention module from that data\u2019s and the LSTM\u2019s input layers. Therefore, when constructing the initial caption data, the LSTM ability to fit caption data to images can be maximized.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6959a37b08ad8db7174cb4755e2fc507",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        853.3,
                        1546.5
                    ],
                    [
                        853.3,
                        1653.5
                    ],
                    [
                        1553.6,
                        1653.5
                    ],
                    [
                        1553.6,
                        1546.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94179,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8,
            "parent_id": "87dbcef1c2e1c62d2b597df11058eb4a"
        },
        "text": "Based on the results presented in Fig. 8, the proposed model converged within 100 epochs, which made model training more effi- cient. Because the CNN-RNN model performed poorly, as shown in Table 3, its training process was not considered.",
        "type": "NarrativeText"
    },
    {
        "element_id": "793aca60faefef808570028281216ab6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        143.8,
                        1747.3
                    ],
                    [
                        143.8,
                        2015.1
                    ],
                    [
                        1495.9,
                        2015.1
                    ],
                    [
                        1495.9,
                        1747.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84736,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8
        },
        "text": "soft attention model \u2018 \u2014 RMSProp Ir = .0003 loss soft attention model \u2018epochs \u2014 Adam Ir = 0.001 \u2014 Adam Ir = 0.003 \u2014 Adam Ir = 0.0003 \u2018 \u2014 Adam Ir = 0.0003 loss epochs",
        "type": "Image"
    },
    {
        "element_id": "20beb48b0de5368d6a4b8323fad9489f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        453.9,
                        2042.7
                    ],
                    [
                        453.9,
                        2061.9
                    ],
                    [
                        1196.6,
                        2061.9
                    ],
                    [
                        1196.6,
                        2042.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.85316,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 8
        },
        "text": "Fig. 8. Training losses of the soft-attention model and the proposed AGNet model.",
        "type": "FigureCaption"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "5c3e16d7efb7a475774d40f62b6d323c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.5,
                        99.2
                    ],
                    [
                        101.5,
                        119.9
                    ],
                    [
                        195.1,
                        119.9
                    ],
                    [
                        195.1,
                        99.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.7309,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 9
        },
        "text": "F. Wuet al.",
        "type": "Header"
    },
    {
        "element_id": "a7def14372f1a3e74a3641119ddd3c0a",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1128.5,
                        100.8
                    ],
                    [
                        1128.5,
                        119.1
                    ],
                    [
                        1550.9,
                        119.1
                    ],
                    [
                        1550.9,
                        100.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.71396,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 9
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "7b2c1aea49339b304d8d80dcd53d80dd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        142.8,
                        131.7
                    ],
                    [
                        142.8,
                        1700.3
                    ],
                    [
                        1530.0,
                        1700.3
                    ],
                    [
                        1530.0,
                        131.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.84986,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 9,
            "parent_id": "a7def14372f1a3e74a3641119ddd3c0a",
            "text_as_html": "<table><thead><tr><th>Image</th><th>Ground truth</th><th>AGNet</th><th>AGNet: No similarity</th><th>Soft attention</th></tr></thead><tbody><tr><td rowspan=\"2\"></td><td>RBCS, BEI FFA LAR HE. FE BE aR BAM RAB id.</td><td>ABLE, FERRE FAM AE TE. FE BEE A A TR i.</td><td>APACE, SERS PAAR HE. FU BE AR i.</td><td>REALE, HEIR JANA AR YE \u00bb FEBS BB \u2014 A A HR Bel.</td></tr><tr><td>Hyperkeratosis of epidermis and liquefying degeneration of basal cells. Band infiltration of monocytes in the upper dermis.</td><td>Hyperkeratosis of epidermis and liquefying degeneration of basal cells, Band infiltration of monocytes in the upper dermis.</td><td>Hyperkeratosis of epidermis and liquefying degeneration of basal cells. Band infiltration of monocytes in the upper dermis.</td><td>Hyperkeratosis of epidermis and liquefying degeneration of basal cells. Band infiltration of monocytes in the upper dermis.</td></tr><tr><td rowspan=\"3\"></td><td>\u2014 Bie te F Be TH I , FEL Be AK ABBR, FRA AT HUA SEEM i A PY Bea, FETA ICE BAA. SRE Hee 2 A i\u00bb</td><td>JL Rede F Be HK tek i FY RA Fl AA me, HE ee be 1A ic HAAR, FCA BY SL ST HE METAL BR 6</td><td>SL Beit F Be LH. ela AC tik 4 Be 9 SH me Hee 1 8 vl A ic RAB, FA By ST EYE TAL Bia 0</td><td>RRA WK He FE RR AY A Hid.</td></tr><tr><td>In the dermis, a large number of capillary and vascular endothelial cells were found, among which a large number of tissue cells, multi-nucleated giant cells, and inflammatory cells were scattered.</td><td>There is a swelling on the skin, which is composed of a large number of blood vessels and inflammatory cells. Its basal _\u2014_ epithelium contracts inward into a ring shape, and fibrous septa can be seen in it.</td><td>There is a swelling on the skin, which is composed of a large number of blood vessels and inflammatory cells. Its basal _\u2014_ epithelium contracts inward into a ring shape, and fibrous septa can be seen in it.</td><td>A large number of Pazhe cells with rich cytoplasm and light staining were found in the whole epidermis.</td></tr><tr><td>FE BE A OL JA We 20 \u00bb FASE AA Jal A ERE AA, SLICE TAL Bat 6 If</td><td>FU A SL FH FR a Hd A, HH SE eR PE HL, J Bl 2H fl SK HE 9, SUNS 2 TA) Bik 2 a BE af |</td><td>FUE A SLA a A TA, FE Jee JER EA 2, J Fe] AH fd 8 SR HE, aa] |</td><td>RRB, A FEAF A HZ Jad A ERAS, 1 Se eT Dik De BL IT a &amp;</td></tr><tr><td></td><td>In dermis, tumor cell mass is composed of basal cells, and the surrounding cells are arranged in palisade shape, with contraction space.</td><td>Ae In dermis, tumor cell mass is composed of basal like cells. The surrounding cells are arranged in __ palisade shape, with contraction space and mucus deposition.</td><td>lc In dermis, tumor cell mass is composed of basal like cells. The surrounding cells are arranged in __ palisade shape, with contraction space and mucus deposition.</td><td>yo Part of the cell mass of the epidermis is composed of basal like cells. The surrounding cells are arranged in palisade, with contraction space and mucus deposition.</td></tr><tr><td></td><td>ARAKE REM A, TAA WEE, AR AY Sit Ro</td><td>ARARASIA Sid, A LBA Sk LR</td><td>ARAM, PAE CREAN ne, JA) Fal 2 AR EA, HY LM iV Bik BLT ae</td><td>= EE SR, AR LS AR</td></tr><tr><td></td><td>There are a lot of spindle cells in dermis and mucoid changes in stroma, without obvious capsule.</td><td>There were _ several Schwann like cells in dermis without obvious capsule.</td><td>In dermis, tumor cell massiscomposedofbasal like cells. The surrounding cells are arranged in __ palisade shape, with contraction</td><td>There were several Schwann like cells in dermis without obvious capsule.</td></tr></tbody></table>"
        },
        "text": "Image Ground truth AGNet AGNet: No similarity Soft attention HBP, REIS FRAN . FU BE i. ABLE, FERRE PAPAL ARTE. FURS if. APACE, SERS PAAR HE. FU BE REALE, HEIR JRA AR HE. FU BE Hyperkeratosis of Hyperkeratosis of epidermis and liquefying epidermis and liquefying degeneration of basal degeneration of basal cells. Band infiltration of monocytes in the upper dermis. cells, Band infiltration of monocytes in the upper dermis. Hyperkeratosis of epidermis and liquefying degeneration of basal cells. Band infiltration of monocytes in the upper dermis. Hyperkeratosis of epidermis and liquefying degeneration of basal cells. Band infiltration of monocytes in the upper dermis. J, \u2014 Bae ike Pe TT I , FEL Be AK ABBR, FRA AT JL Rede F Be a Aik Be 9 SH me, HE ee be 1A ic HUA SEEM i A PY Bea, FETA ICE RAM. SRAM Hee 2 A i\u00bb RMR, FEA BTL SL Beit F Be LH. ela AC tik 4 Be 9 SH me Hee 1 8 vl A ic RAB, FA By ST EYE TAL Bia 0 RRA WK Ee FE RR AY A Hid. In the dermis, a large number of capillary and vascular endothelial cells were found, among which a large number of tissue cells, multi-nucleated giant cells, and inflammatory cells were scattered. There is a swelling on the skin, which is composed of a large number of blood vessels and inflammatory cells. Its basal _\u2014_ epithelium contracts inward into a ring shape, and fibrous septa can be seen in it. There is a swelling on the skin, which is composed of a large number of blood vessels and inflammatory cells. Its basal _\u2014_ epithelium contracts inward into a ring shape, and fibrous septa can be seen in it. A large number of Pazhe cells with rich cytoplasm and light staining were found in the whole epidermis. FE BA SLA RE A A BAL RL, Al, HAE Re AMZ A. Jad RAR HL EL, Jad 6] 2 ed SR HE A, Bl 2H fl SK HE 9, HY Sc TA Bak TAY LMSC 24 VY Bk 2 BE af Ae FUE A SLA a A TA, FE Jee JER EA 2, J Fe] AH fd 8 SR HE, evel wea bene aa] lc RRB, A FEAF A HZ Jad A PEARS, AT Se eT Dik De BL IT a yo & In dermis, tumor cell mass is composed of basal cells, and the surrounding cells are arranged in palisade In dermis, tumor cell mass is composed of basal like cells. The surrounding cells are arranged in __ palisade In dermis, tumor cell mass is composed of basal like cells. The surrounding cells are arranged in __ palisade Part of the cell mass of the epidermis is composed of basal like cells. The surrounding cells are arranged in shape, with contraction shape, with contraction shape, with contraction palisade, with space. space and mucus space and mucus contraction space and deposition. deposition. mucus deposition. ARAKE REM ARARASIA ARAM, = EE A, TAA WEE, AR Sid, A LBA Sk LR PAE CREAN ne, JA) SR, AR LS AR AY Sit Ro Fal 21 fl FASE HY LM iV Bik BLT a There are a lot of spindle cells in dermis and mucoid changes in stroma, without obvious capsule. There were several Schwann like cells in dermis without obvious capsule. In dermis, tumor cell mass is composed of basal like cells. The surrounding cells are arranged in __ palisade shape, with contraction space and mucus There were several Schwann like cells in dermis without obvious capsule.",
        "type": "Table"
    },
    {
        "element_id": "920233df820fff60b9d8bc0f8e5f21b6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        994.0,
                        1706.0
                    ],
                    [
                        994.0,
                        1742.0
                    ],
                    [
                        1098.0,
                        1742.0
                    ],
                    [
                        1098.0,
                        1706.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 9,
            "parent_id": "a7def14372f1a3e74a3641119ddd3c0a"
        },
        "text": "deposition.",
        "type": "Title"
    },
    {
        "element_id": "e50f88f650f2b7e2773a7f977df94b60",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        599.3,
                        1763.9
                    ],
                    [
                        599.3,
                        1784.8
                    ],
                    [
                        1046.9,
                        1784.8
                    ],
                    [
                        1046.9,
                        1763.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.69062,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 9,
            "parent_id": "920233df820fff60b9d8bc0f8e5f21b6"
        },
        "text": "Fig. 9. Results obtained by three different models.",
        "type": "FigureCaption"
    },
    {
        "element_id": "891f4391fc6a8d67f7a89b65984bf17f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        107.4,
                        1826.0
                    ],
                    [
                        107.4,
                        1846.2
                    ],
                    [
                        322.3,
                        1846.2
                    ],
                    [
                        322.3,
                        1826.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.46476,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 9,
            "parent_id": "920233df820fff60b9d8bc0f8e5f21b6"
        },
        "text": "4.5. Qualitative results",
        "type": "ListItem"
    },
    {
        "element_id": "f4392bcfe158090818b5945c2aefe65f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.3,
                        1884.9
                    ],
                    [
                        105.3,
                        1905.2
                    ],
                    [
                        381.4,
                        1905.2
                    ],
                    [
                        381.4,
                        1884.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.43672,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 9,
            "parent_id": "a7def14372f1a3e74a3641119ddd3c0a"
        },
        "text": "4.5.1. Paragraph generation",
        "type": "Title"
    },
    {
        "element_id": "8c6f385d03add935f11e065726ec3d4d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.8,
                        1913.8
                    ],
                    [
                        103.8,
                        2049.7
                    ],
                    [
                        805.5,
                        2049.7
                    ],
                    [
                        805.5,
                        1913.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94315,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 9,
            "parent_id": "f4392bcfe158090818b5945c2aefe65f"
        },
        "text": "In Fig. 9, the image caption results obtained by the AGNet model, the AGNet model without similarity, and the soft-attention model are pre- sented. It should be noted that the caption data of the SPI dataset were fairly uncomplicated. Due to the similarity between features extracted from images of the same category, there was a certain repetition level in",
        "type": "NarrativeText"
    },
    {
        "element_id": "ca6eb0a99dfdf26fae7d7a918e7316a2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        849.5,
                        1825.5
                    ],
                    [
                        849.5,
                        2049.3
                    ],
                    [
                        1547.1,
                        2049.3
                    ],
                    [
                        1547.1,
                        1825.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95269,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 9,
            "parent_id": "f4392bcfe158090818b5945c2aefe65f"
        },
        "text": "the corresponding caption data. The comparison with the ground truth data indicated that the caption data consisted mainly of technical vo- cabulary and had a rigid format. Once the expressions had been segmented, the sentence structure of the caption data was \u201csomething plus a corresponding adjectival description.\u201d The expression segmenting process regarded many word pairs as a single descriptor, while a tumor cell was considered an expression. This mechanism reduced the number of parameters in the embedded matrix of the caption data, which",
        "type": "NarrativeText"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "c8a672872ef8ee3b21d12770e130ad53",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        101.3,
                        99.1
                    ],
                    [
                        101.3,
                        119.8
                    ],
                    [
                        195.4,
                        119.8
                    ],
                    [
                        195.4,
                        99.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.74146,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "F. Wuet al.",
        "type": "Header"
    },
    {
        "element_id": "6ad348d4b702d971a15ce1edcbeed139",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1128.8,
                        101.2
                    ],
                    [
                        1128.8,
                        118.9
                    ],
                    [
                        1550.5,
                        118.9
                    ],
                    [
                        1550.5,
                        101.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.79728,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Header"
    },
    {
        "element_id": "0342041ebeb89cf2a4f20697c7c188a6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        185.9,
                        158.7
                    ],
                    [
                        185.9,
                        812.1
                    ],
                    [
                        1477.4,
                        812.1
                    ],
                    [
                        1477.4,
                        158.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93312,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "OES FA BE FL Sk hh LB Be BK Hi BK ff Bate oii. FEAR Ye APA PE In dermal papilla, there are there are lumps staining positive pink stained lumps. Gentian violet staining was positive. FUE AY Be dl SHE HL, AR LBA Se LR There were several Schwann. like cells in dermis without obvious capsule. | Se Schwann like cells AR am without capsule",
        "type": "Image"
    },
    {
        "element_id": "a3e08bd51e7ef6db13a65b37cf023a7e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        678.0,
                        840.9
                    ],
                    [
                        678.0,
                        863.5
                    ],
                    [
                        973.0,
                        863.5
                    ],
                    [
                        973.0,
                        840.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.78471,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "Fig. 10. Visual attention results.",
        "type": "FigureCaption"
    },
    {
        "element_id": "549a72df9806113ec341cb29c09b2ab9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        100.4,
                        904.4
                    ],
                    [
                        100.4,
                        924.9
                    ],
                    [
                        570.2,
                        924.9
                    ],
                    [
                        570.2,
                        904.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.65153,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "significantly improved the model performance.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f2e7e64a1a92f51dbdf4eca488874c2f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.2,
                        901.4
                    ],
                    [
                        855.2,
                        953.8
                    ],
                    [
                        1551.2,
                        953.8
                    ],
                    [
                        1551.2,
                        901.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91081,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "learning framework to other similar medical imaging diagnostic report generation applications.",
        "type": "NarrativeText"
    },
    {
        "element_id": "fb6afb2864d3db37ff6bddbf3e2432d3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        104.3,
                        962.1
                    ],
                    [
                        104.3,
                        982.5
                    ],
                    [
                        345.9,
                        982.5
                    ],
                    [
                        345.9,
                        962.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.41664,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "4.5.2. Attention learning",
        "type": "Title"
    },
    {
        "element_id": "cb1c2273202bfabb2bef26a79a8b663d",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.5,
                        990.7
                    ],
                    [
                        105.5,
                        1422.3
                    ],
                    [
                        806.5,
                        1422.3
                    ],
                    [
                        806.5,
                        990.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9537,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "fb6afb2864d3db37ff6bddbf3e2432d3"
        },
        "text": "The effect of the visual attention module is shown in Fig. 10, where red and blue colors correspond to the focus area and the cold point, respectively. Guided by the word generation at each step of the language model, the similarity-attention module output an attention region based on image features. For images in the same category, the attention module recognized similar image features and guided the LSTM toward generating an appropriate word. More specifically, the module gener- ated a one-to-one matching between the attention areas obtained by visualization and the words generated by the LSTM. The similarity- attention module was then oriented toward the correct feature area using gradient descent to minimize the LSTM output loss. The visualized output results of the similarity-attention module were examined by experienced radiologists and received the feedback that they had been identical regarding the diagnostic criteria, such as relevant tissue, mass, and radiologic features.",
        "type": "NarrativeText"
    },
    {
        "element_id": "7e9f2ebc14a63f3c812d03b264005886",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.3,
                        1016.9
                    ],
                    [
                        854.3,
                        1041.1
                    ],
                    [
                        1212.8,
                        1041.1
                    ],
                    [
                        1212.8,
                        1016.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8158,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "Declaration of competing interest",
        "type": "Title"
    },
    {
        "element_id": "9e6777d6831df95def7c546a5cfcb14f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.3,
                        1074.6
                    ],
                    [
                        855.3,
                        1157.3
                    ],
                    [
                        1549.4,
                        1157.3
                    ],
                    [
                        1549.4,
                        1074.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93149,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "7e9f2ebc14a63f3c812d03b264005886"
        },
        "text": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.",
        "type": "NarrativeText"
    },
    {
        "element_id": "ecdd91731dbafa16c06bf826a02fa1a4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.1,
                        1199.4
                    ],
                    [
                        851.1,
                        1222.4
                    ],
                    [
                        1046.2,
                        1222.4
                    ],
                    [
                        1046.2,
                        1199.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81948,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "Acknowledgments",
        "type": "Title"
    },
    {
        "element_id": "0cecb1cb375071a29dc85440d2a2848c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.5,
                        1255.2
                    ],
                    [
                        856.5,
                        1482.9
                    ],
                    [
                        1552.8,
                        1482.9
                    ],
                    [
                        1552.8,
                        1255.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.95402,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "ecdd91731dbafa16c06bf826a02fa1a4"
        },
        "text": "This work was supported in part by the National Nature Science and Foundation of China under Grant No. 71801031, the Guangdong Basic and Applied Basic Research Foundation of China project \u201cResearch on dermatosis automatic diagnosis system based on multi-type of medical image\u201d under Grant No. 2019A1515011962, and the Science and Technology Innovation Strategy Foundation of Guangdong Province under Grant No. pdjh2020b0013. We thank LetPub (www.letpub.com) for its linguistic assistance during the preparation of this manuscript.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d6029d4ab9e054790101542e4c5de3d4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        103.1,
                        1462.6
                    ],
                    [
                        103.1,
                        1482.6
                    ],
                    [
                        255.7,
                        1482.6
                    ],
                    [
                        255.7,
                        1462.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.83415,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "5. Conclusion",
        "type": "Title"
    },
    {
        "element_id": "12cbbebb1d05c88ab805cc7fafe8dcda",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        100.3,
                        1519.0
                    ],
                    [
                        100.3,
                        2059.4
                    ],
                    [
                        803.8,
                        2059.4
                    ],
                    [
                        803.8,
                        1519.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.94853,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "d6029d4ab9e054790101542e4c5de3d4"
        },
        "text": "This paper studies the automatic generation of diagnosis reports from skin pathology images. Considering three principal challenges when generating diagnostical reports, namely ensuring report accuracy and fast report generation and visualizing the basis for generated di- agnoses, the AGNet model is constructed. The AGNet model combines deep learning and attention mechanism. In particular, the self-attention mechanism is used in the proposed model, which is a milestone in the field of natural language processing, and a similarity-attention mecha- nism is designed to locate the correct image attention area in the process of text generation. For that purpose, a special mode is used for con- necting the attention module. Since the proposed CNN-based model is an extremely accurate classifier of images in the pretraining process, the output label of the image model can be fully used; therefore, an embedding and labeling module is introduced. The proposed model is verified on the skin pathological image data, and the obtained results have verified both the effectiveness and the robustness of the proposed model. For future work, we will construct another skin pathological image dataset with larger scale, to further validate the effectiveness of our Algorithm and refine it. We also attempt to generalize this deep",
        "type": "NarrativeText"
    },
    {
        "element_id": "66e91b00c02050ae2333be985de2c433",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.8,
                        1527.1
                    ],
                    [
                        851.8,
                        1547.4
                    ],
                    [
                        967.2,
                        1547.4
                    ],
                    [
                        967.2,
                        1527.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.81656,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "References",
        "type": "Title"
    },
    {
        "element_id": "c1f304773ddfc0e914be110ef455bd31",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        863.9,
                        1578.2
                    ],
                    [
                        863.9,
                        1620.8
                    ],
                    [
                        1524.4,
                        1620.8
                    ],
                    [
                        1524.4,
                        1578.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90086,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "66e91b00c02050ae2333be985de2c433"
        },
        "text": "[1] N. Wu, et al., Deep neural networks improve radiologists\u2019 performance in breast cancer screening, IEEE Trans. Med. Imag. 39 (4) (2020) 1184-1194.",
        "type": "ListItem"
    },
    {
        "element_id": "628c31af9df9d353f3052ac506cb7c86",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        854.3,
                        1622.8
                    ],
                    [
                        854.3,
                        1664.4
                    ],
                    [
                        1515.6,
                        1664.4
                    ],
                    [
                        1515.6,
                        1622.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.8833,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "66e91b00c02050ae2333be985de2c433"
        },
        "text": "S. Feng, et al., CPFNet: context pyramid fusion network for medical image segmentation, IEEE Trans. Med. Imag. (2020) 1, 1. [2]",
        "type": "ListItem"
    },
    {
        "element_id": "ba8a269dd7aa7f8b0d02a09695298eae",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        867.5,
                        1666.8
                    ],
                    [
                        867.5,
                        1709.1
                    ],
                    [
                        1536.3,
                        1709.1
                    ],
                    [
                        1536.3,
                        1666.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90772,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "66e91b00c02050ae2333be985de2c433"
        },
        "text": "M. Corbetta, G.L. Shulman, Control of goal-directed and stimulus-driven attention in the brain, Nat. Rev. Neurosci. 3 (3) (2002) 201-215. [3]",
        "type": "ListItem"
    },
    {
        "element_id": "89424d90d698e3a8826e4d6dc802ff03",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        868.0,
                        1711.5
                    ],
                    [
                        868.0,
                        1753.5
                    ],
                    [
                        1534.5,
                        1753.5
                    ],
                    [
                        1534.5,
                        1711.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89773,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "66e91b00c02050ae2333be985de2c433"
        },
        "text": "R.A. Rensink, The dynamic representation of scenes, Vis. Cognit. 7 (1-3) (2000) 17-42. [4]",
        "type": "ListItem"
    },
    {
        "element_id": "15d5e5332d20f4d251e0180f6f42422b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        864.0,
                        1755.7
                    ],
                    [
                        864.0,
                        1818.9
                    ],
                    [
                        1537.3,
                        1818.9
                    ],
                    [
                        1537.3,
                        1755.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92042,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "66e91b00c02050ae2333be985de2c433"
        },
        "text": "G.N. Gunesli, C. Sokmensuer, C. Gunduz-Demir, Attention Boost: learning what to attend for gland segmentation in histopathological images by boosting fully convolutional networks, IEEE Trans. Med. Imag. (2020) 1, 1. [5]",
        "type": "ListItem"
    },
    {
        "element_id": "6be464e15a19347467caee895b375a36",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        860.9,
                        1821.7
                    ],
                    [
                        860.9,
                        1886.0
                    ],
                    [
                        1545.4,
                        1886.0
                    ],
                    [
                        1545.4,
                        1821.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92435,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "66e91b00c02050ae2333be985de2c433"
        },
        "text": "M. Li, et al., SACNN: self-attention convolutional neural network for low-dose CT denoising with self-supervised perceptual loss network, IEEE Trans. Med. Imag. 39 (7) (2020) 2289-2301. [6]",
        "type": "ListItem"
    },
    {
        "element_id": "dea6d2509ed015d30bda0bc1aff00ae5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        865.4,
                        1887.7
                    ],
                    [
                        865.4,
                        1930.7
                    ],
                    [
                        1533.3,
                        1930.7
                    ],
                    [
                        1533.3,
                        1887.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90719,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "66e91b00c02050ae2333be985de2c433"
        },
        "text": "K. Xu, et al., Show, attend and tell: neural image caption generation with visual attention, in: International Conference on Machine Learning, 2015. 7]",
        "type": "ListItem"
    },
    {
        "element_id": "b1fdc19420ae79fe0445d9f948bbef8b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        866.0,
                        1933.3
                    ],
                    [
                        866.0,
                        1997.0
                    ],
                    [
                        1501.4,
                        1997.0
                    ],
                    [
                        1501.4,
                        1933.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9177,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "66e91b00c02050ae2333be985de2c433"
        },
        "text": "L. Chen, et al., Sca-cnn: spatial and channel-wise attention in convolutional networks for image captioning, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. [8]",
        "type": "ListItem"
    },
    {
        "element_id": "e2f838df7fe83dcf062b78fa105134ae",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        865.9,
                        1999.5
                    ],
                    [
                        865.9,
                        2062.2
                    ],
                    [
                        1533.5,
                        2062.2
                    ],
                    [
                        1533.5,
                        1999.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91953,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10,
            "parent_id": "66e91b00c02050ae2333be985de2c433"
        },
        "text": "J. Lu, et al., Knowing when to look: adaptive attention via a visual sentinel for image captioning, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. [9]",
        "type": "ListItem"
    },
    {
        "element_id": "56f154c508920eb6df83cd067b32d196",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        818.2,
                        2101.0
                    ],
                    [
                        818.2,
                        2120.6
                    ],
                    [
                        838.4,
                        2120.6
                    ],
                    [
                        838.4,
                        2101.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.70305,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 10
        },
        "text": "10",
        "type": "Footer"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "632815595e9bc328b2a8251f34667743",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        102.4,
                        100.0
                    ],
                    [
                        102.4,
                        119.5
                    ],
                    [
                        194.5,
                        119.5
                    ],
                    [
                        194.5,
                        100.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.75623,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11
        },
        "text": "F. Wuet al.",
        "type": "Title"
    },
    {
        "element_id": "934be84486c0bab9c638f1bebfae201e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        107.8,
                        151.1
                    ],
                    [
                        107.8,
                        215.0
                    ],
                    [
                        792.8,
                        215.0
                    ],
                    [
                        792.8,
                        151.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9154,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[10] Z. Yang, et al., Stacked attention networks for image question answering, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "f560dd77de6d5dc3a255bd016f1c7fa0",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        106.1,
                        217.9
                    ],
                    [
                        106.1,
                        281.0
                    ],
                    [
                        789.7,
                        281.0
                    ],
                    [
                        789.7,
                        217.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92369,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "1] D. Yu, et al., Multi-level attention networks for visual question answering, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "5eb8ffba28a3cc910c9fc4162fa28c42",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        105.7,
                        283.8
                    ],
                    [
                        105.7,
                        347.6
                    ],
                    [
                        784.3,
                        347.6
                    ],
                    [
                        784.3,
                        283.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93172,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "12] P. Anderson, et al., Bottom-up and top-down attention for image captioning and visual question answering, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "677e1cf2b72fb05b1d98021ddefa50d2",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        110.9,
                        350.6
                    ],
                    [
                        110.9,
                        392.5
                    ],
                    [
                        787.9,
                        392.5
                    ],
                    [
                        787.9,
                        350.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90943,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "13] A. Vaswani, et al., Attention is all you need, in: Advances in Neural Information Processing Systems, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "c199ab4680309f4d1c5c706254e9321e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        109.8,
                        395.6
                    ],
                    [
                        109.8,
                        436.0
                    ],
                    [
                        785.8,
                        436.0
                    ],
                    [
                        785.8,
                        395.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90281,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "14] H. Hu, et al., Relation networks for object detection, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "e702dd3a3f68f2b5d5782fcf79fa6869",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        112.2,
                        438.4
                    ],
                    [
                        112.2,
                        480.3
                    ],
                    [
                        791.2,
                        480.3
                    ],
                    [
                        791.2,
                        438.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9053,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[15] L. Huang, et al., Attention on attention for image captioning, in: Proceedings of the IEEE International Conference on Computer Vision, 2019.",
        "type": "ListItem"
    },
    {
        "element_id": "a197234561f8259f511aa0ce12735bb4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        109.0,
                        483.8
                    ],
                    [
                        109.0,
                        525.2
                    ],
                    [
                        797.4,
                        525.2
                    ],
                    [
                        797.4,
                        483.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90711,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[16] X. Wang, et al., Non-local neural networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "a7a4988ab17efa0c5174331270cb9d37",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        114.2,
                        527.8
                    ],
                    [
                        114.2,
                        591.1
                    ],
                    [
                        799.4,
                        591.1
                    ],
                    [
                        799.4,
                        527.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92977,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "07] H.-C. Shin, et al., Learning to read chest x-rays: recurrent neural cascade model for automated image annotation, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "b9f648e7686d555ecab95ce4e3f6f753",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        115.3,
                        593.5
                    ],
                    [
                        115.3,
                        634.8
                    ],
                    [
                        768.5,
                        634.8
                    ],
                    [
                        768.5,
                        593.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90505,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "18] B. Jing, P. Xie, E. Xing, On the automatic generation of medical imaging reports, 2017 arXiv preprint arXiv:1711.08195.",
        "type": "ListItem"
    },
    {
        "element_id": "dba4503619784a4dbf2e34ae42c13935",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        106.6,
                        639.3
                    ],
                    [
                        106.6,
                        680.1
                    ],
                    [
                        742.4,
                        680.1
                    ],
                    [
                        742.4,
                        639.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90836,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "19] L. Kaiser, et al., One model to learn them all, 2017 arXiv preprint arXiv: 1706.05137.",
        "type": "ListItem"
    },
    {
        "element_id": "7e0e09413bc938a90d5b13dac8dfb343",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        106.8,
                        682.7
                    ],
                    [
                        106.8,
                        746.0
                    ],
                    [
                        785.8,
                        746.0
                    ],
                    [
                        785.8,
                        682.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91968,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[20] J. Tian, et al., A diagnostic report generator from CT volumes on liver tumor with semi-supervised attention mechanism, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "496107fdf46d1c8f4e0da1543fa7afcd",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        112.2,
                        748.9
                    ],
                    [
                        112.2,
                        790.1
                    ],
                    [
                        782.4,
                        790.1
                    ],
                    [
                        782.4,
                        748.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90417,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[21] AK. Vijayakumar, et al., Diverse beam search: decoding diverse solutions from neural sequence models, 2016 arXiv preprint arXiv:1610.02424.",
        "type": "ListItem"
    },
    {
        "element_id": "56a280b89df4b6ce1a1df3feb177c4bb",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        107.9,
                        793.8
                    ],
                    [
                        107.9,
                        835.2
                    ],
                    [
                        794.9,
                        835.2
                    ],
                    [
                        794.9,
                        793.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90868,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[22] R.R. Selvaraju, et al., Grad-CAM: why did you say that?, 2016 arXiv preprint arXi 1611.07450.",
        "type": "ListItem"
    },
    {
        "element_id": "dffe46b3a87f732ff4ff0e79f8806711",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        114.8,
                        837.3
                    ],
                    [
                        114.8,
                        900.9
                    ],
                    [
                        794.2,
                        900.9
                    ],
                    [
                        794.2,
                        837.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93167,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[23] Y. Xue, et al., Multimodal recurrent model with attention for automated radiology report generation, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, 2018.",
        "type": "ListItem"
    },
    {
        "element_id": "1152628da8ceeb258546ccef0f00bc8f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        113.4,
                        903.8
                    ],
                    [
                        113.4,
                        945.4
                    ],
                    [
                        777.5,
                        945.4
                    ],
                    [
                        777.5,
                        903.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9105,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[24] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Comput. 9 (8) (1997) 1735-1780.",
        "type": "ListItem"
    },
    {
        "element_id": "4ad572fdb4a2d4f7204df6941e1d7a21",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        111.1,
                        947.9
                    ],
                    [
                        111.1,
                        989.4
                    ],
                    [
                        780.9,
                        989.4
                    ],
                    [
                        780.9,
                        947.9
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90955,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[25] X. Yang, et al., Auto-encoding scene graphs for image captioning, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.",
        "type": "ListItem"
    },
    {
        "element_id": "43f3b463dfdd04ab3db4c4e55f1f2211",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        110.8,
                        991.7
                    ],
                    [
                        110.8,
                        1033.6
                    ],
                    [
                        790.5,
                        1033.6
                    ],
                    [
                        790.5,
                        991.7
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90214,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[26] T. Yao, et al., Boosting image captioning with attributes, in: Proceedings of the IEEE International Conference on Computer Vision, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "fd133cc29dacb1059eceb07be13de446",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        112.9,
                        1037.4
                    ],
                    [
                        112.9,
                        1078.2
                    ],
                    [
                        796.5,
                        1078.2
                    ],
                    [
                        796.5,
                        1037.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90826,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[27] T. Yao, et al., Exploring visual relationship for image captioning, i the European Conference on Computer Vision, ECCV), 2018. : Proceedings of",
        "type": "ListItem"
    },
    {
        "element_id": "ed0749b90783e6b3bffe63a54c068815",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        110.0,
                        1080.8
                    ],
                    [
                        110.0,
                        1121.7
                    ],
                    [
                        790.2,
                        1121.7
                    ],
                    [
                        790.2,
                        1080.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90084,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[28] M. Yang, et al., Multitask learning for cross-domain image captioning, IEEE Trans. Multimed. 21 (4) (2019) 1047-1061.",
        "type": "ListItem"
    },
    {
        "element_id": "35173d0f3e235a12b8bc32bbbee1193b",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        111.9,
                        1126.0
                    ],
                    [
                        111.9,
                        1166.4
                    ],
                    [
                        754.7,
                        1166.4
                    ],
                    [
                        754.7,
                        1126.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89962,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[29] T. Wang, et al., Visual commonsense r-cnn, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.",
        "type": "ListItem"
    },
    {
        "element_id": "97f0db76d48d9ecf63eb6d68ed77f1ca",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        111.1,
                        1169.4
                    ],
                    [
                        111.1,
                        1210.9
                    ],
                    [
                        724.1,
                        1210.9
                    ],
                    [
                        724.1,
                        1169.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88957,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "632815595e9bc328b2a8251f34667743"
        },
        "text": "[30] Y.J. Tian, S.J. Fu, A descriptive framework for the field of deep learning applications in medical images, Knowl. Base Syst. 210 (2020) 22.",
        "type": "ListItem"
    },
    {
        "element_id": "2f7de71b850bea6cb007559be3699db5",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        817.4,
                        2101.0
                    ],
                    [
                        817.4,
                        2121.0
                    ],
                    [
                        837.6,
                        2121.0
                    ],
                    [
                        837.6,
                        2101.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.56903,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11
        },
        "text": "11",
        "type": "Footer"
    },
    {
        "element_id": "18ad8093f136b2a70a994b9e1af8aad1",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        1127.3,
                        100.4
                    ],
                    [
                        1127.3,
                        119.1
                    ],
                    [
                        1556.1,
                        119.1
                    ],
                    [
                        1556.1,
                        100.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.75665,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11
        },
        "text": "Computers in Biology and Medicine 141 (2022) 105037",
        "type": "Title"
    },
    {
        "element_id": "5577697f037e34522b0cf0abdee76027",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        859.1,
                        151.0
                    ],
                    [
                        859.1,
                        215.3
                    ],
                    [
                        1535.3,
                        215.3
                    ],
                    [
                        1535.3,
                        151.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92636,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[31] Y.D. Chai, H.Y. Liu, J. Xu, Glaucoma diagnosis based on both hidden features and domain knowledge through deep learning models, Knowl. Base Syst. 161 (2018) 147-156.",
        "type": "ListItem"
    },
    {
        "element_id": "968b05f11cd95b38534ef6d4a50ceaa4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        855.2,
                        218.2
                    ],
                    [
                        855.2,
                        259.5
                    ],
                    [
                        1523.2,
                        259.5
                    ],
                    [
                        1523.2,
                        218.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90844,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[32] G. Nir, et al., Automatic grading of prostate cancer in digitized histopathology images: learning from multiple experts, Med. Image Anal. 50 (2018) 167-180.",
        "type": "ListItem"
    },
    {
        "element_id": "0216eaa8d3085fe0b708a8768b51fc47",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.0,
                        262.6
                    ],
                    [
                        856.0,
                        303.2
                    ],
                    [
                        1547.5,
                        303.2
                    ],
                    [
                        1547.5,
                        262.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.88973,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[33] C. Spampinato, et al., Deep learning for automated skeletal bone age assessment in X-ray images, Med. Image Anal. 36 (2017) 41-51.",
        "type": "ListItem"
    },
    {
        "element_id": "70f6fa15074cc298360fe8e82bed7f20",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        851.3,
                        305.6
                    ],
                    [
                        851.3,
                        325.1
                    ],
                    [
                        1407.0,
                        325.1
                    ],
                    [
                        1407.0,
                        305.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.72617,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[34] Z. Swiderska-Chadaj, et al., Learning to detect lymphocytes in",
        "type": "ListItem"
    },
    {
        "element_id": "cfc41019a738c0fb79d17bd39c589360",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        865.3,
                        327.5
                    ],
                    [
                        865.3,
                        348.0
                    ],
                    [
                        1538.6,
                        348.0
                    ],
                    [
                        1538.6,
                        327.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.76283,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "immunohistochemistry with deep learning, Med. Image Anal. 58 (2019) 101547.",
        "type": "ListItem"
    },
    {
        "element_id": "09992967039376a0ecd8a32a3214c8ec",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.0,
                        350.4
                    ],
                    [
                        856.0,
                        393.2
                    ],
                    [
                        1542.5,
                        393.2
                    ],
                    [
                        1542.5,
                        350.4
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90542,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[35] E. Pesce, et al., Learning to detect chest radiographs containing pulmonary lesions using visual attention networks, Med. Image Anal. 53 (2019) 26-38.",
        "type": "ListItem"
    },
    {
        "element_id": "7ee3f48dffddd90cb614cdf31c93dea6",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        863.0,
                        395.8
                    ],
                    [
                        863.0,
                        457.8
                    ],
                    [
                        1540.0,
                        457.8
                    ],
                    [
                        1540.0,
                        395.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92733,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[36] Z. Zhang, et al., Mdnet: a semantically and visually interpretable medical image diagnosis network, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "d26fb2a40df5cff95a3b894f6f572e7c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.0,
                        460.5
                    ],
                    [
                        861.0,
                        524.8
                    ],
                    [
                        1537.1,
                        524.8
                    ],
                    [
                        1537.1,
                        460.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93127,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[37] J. Pavlopoulos, V. Kougia, I. Androutsopoulos, A survey on biomedical image captioning, in: Proceedings of the Second Workshop on Shortcomings in Vision and Language, 2019.",
        "type": "ListItem"
    },
    {
        "element_id": "127cc766783240711ce2e8fa7f875a8e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        862.4,
                        527.5
                    ],
                    [
                        862.4,
                        569.3
                    ],
                    [
                        1538.9,
                        569.3
                    ],
                    [
                        1538.9,
                        527.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91708,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[38] E. Rojas, et al., Process mining in healthcare: a literature review, J. Biomed. Inf. 61 (2016) 224-236.",
        "type": "ListItem"
    },
    {
        "element_id": "119e35364b6938299173ed05cbe860d4",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        863.8,
                        572.3
                    ],
                    [
                        863.8,
                        612.8
                    ],
                    [
                        1539.1,
                        612.8
                    ],
                    [
                        1539.1,
                        572.3
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91452,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[39] J.F. Chen, et al., A data-driven framework of typical treatment process extraction and evaluation, J. Biomed. Inf. 83 (2018) 178-195.",
        "type": "ListItem"
    },
    {
        "element_id": "f3b2d612ddb374f2da0b37f9be07f412",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        858.3,
                        615.2
                    ],
                    [
                        858.3,
                        658.0
                    ],
                    [
                        1523.5,
                        658.0
                    ],
                    [
                        1523.5,
                        615.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.89971,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[40] G. Leonardi, et al., Leveraging semantic labels for multi-level abstraction in medical process mining and trace comparison, J. Biomed. Inf. 83 (2018) 10-24.",
        "type": "ListItem"
    },
    {
        "element_id": "9c72552af5fa5cf3da978b98da5d1444",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        860.3,
                        661.1
                    ],
                    [
                        860.3,
                        703.3
                    ],
                    [
                        1541.5,
                        703.3
                    ],
                    [
                        1541.5,
                        661.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.912,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[41] P. Kisilev, et al., From medical image to automatic medical report generation, IBM J. Res. Dev. 59 (2/3) (2015).",
        "type": "ListItem"
    },
    {
        "element_id": "d95360b7bfc41dda68118efa25187a84",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        862.3,
                        705.1
                    ],
                    [
                        862.3,
                        768.1
                    ],
                    [
                        1533.7,
                        768.1
                    ],
                    [
                        1533.7,
                        705.1
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92516,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[42] C. Szegedy, et al., Rethinking the inception architecture for computer vision, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
        "type": "ListItem"
    },
    {
        "element_id": "1affcb214bcbe3ceac82ec6eef5e9359",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.1,
                        770.5
                    ],
                    [
                        856.1,
                        812.2
                    ],
                    [
                        1502.8,
                        812.2
                    ],
                    [
                        1502.8,
                        770.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91666,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[43] D.P. Kingma, J. Ba, Adam: a method for stochastic optimization, 2014 arXiv preprint arXiv:1412.6980.",
        "type": "ListItem"
    },
    {
        "element_id": "edb162dae612ba99e180ed6769266a8c",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        858.3,
                        815.8
                    ],
                    [
                        858.3,
                        856.2
                    ],
                    [
                        1515.2,
                        856.2
                    ],
                    [
                        1515.2,
                        815.8
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90729,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[44] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, 2014 arXiv preprint arXiv:1409.1556.",
        "type": "ListItem"
    },
    {
        "element_id": "b7576e962a5adba83417503d323fa30e",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        861.1,
                        859.0
                    ],
                    [
                        861.1,
                        901.8
                    ],
                    [
                        1539.4,
                        901.8
                    ],
                    [
                        1539.4,
                        859.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.90325,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[45] EN. Iandola, et al., SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size, 2016 arXiv preprint arXiv:1602.07360.",
        "type": "ListItem"
    },
    {
        "element_id": "106f117cc941059b29bff545015b66c3",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        858.1,
                        903.5
                    ],
                    [
                        858.1,
                        945.1
                    ],
                    [
                        1542.9,
                        945.1
                    ],
                    [
                        1542.9,
                        903.5
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.9035,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[46] O. Vinyals, et al., Show and tell: a neural image caption generator, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015.",
        "type": "ListItem"
    },
    {
        "element_id": "f17613a6d496c3fad3280b0bc7961524",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        859.8,
                        947.6
                    ],
                    [
                        859.8,
                        1010.8
                    ],
                    [
                        1536.0,
                        1010.8
                    ],
                    [
                        1536.0,
                        947.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92108,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[47] S.J. Rennie, et al., Self-critical sequence training for image captioning, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
        "type": "ListItem"
    },
    {
        "element_id": "adaea422e84bbb5d4d76f9bcf8907700",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        857.4,
                        1014.2
                    ],
                    [
                        857.4,
                        1077.6
                    ],
                    [
                        1508.6,
                        1077.6
                    ],
                    [
                        1508.6,
                        1014.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93102,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[48] K. Papineni, et al., BLEU: a method for automatic evaluation of machine translation, in: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 2002.",
        "type": "ListItem"
    },
    {
        "element_id": "ceeb723402c7384031329ee34c7c881f",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        860.9,
                        1080.0
                    ],
                    [
                        860.9,
                        1144.2
                    ],
                    [
                        1529.3,
                        1144.2
                    ],
                    [
                        1529.3,
                        1080.0
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.93194,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[49] M. Denkowski, A. Lavie, Meteor universal: language specific translation evaluation for any target language, in: Proceedings of the Ninth Workshop on Statistical Machine Translation, 2014.",
        "type": "ListItem"
    },
    {
        "element_id": "035cd8317f060b1659296d1de909d504",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        856.4,
                        1147.2
                    ],
                    [
                        856.4,
                        1188.2
                    ],
                    [
                        1495.6,
                        1188.2
                    ],
                    [
                        1495.6,
                        1147.2
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.91019,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[50] C.-Y. Lin, Rouge: a package for automatic evaluation of summaries, in: Text Summarization Branches Out, 2004.",
        "type": "ListItem"
    },
    {
        "element_id": "d92769b1ff0434f35ead4fcc310bcfd9",
        "metadata": {
            "coordinates": {
                "layout_height": 2205,
                "layout_width": 1654,
                "points": [
                    [
                        857.2,
                        1191.6
                    ],
                    [
                        857.2,
                        1255.6
                    ],
                    [
                        1518.8,
                        1255.6
                    ],
                    [
                        1518.8,
                        1191.6
                    ]
                ],
                "system": "PixelSpace"
            },
            "detection_class_prob": 0.92227,
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf",
            "languages": [
                "eng"
            ],
            "last_modified": "2024-06-30T16:21:11",
            "page_number": 11,
            "parent_id": "18ad8093f136b2a70a994b9e1af8aad1"
        },
        "text": "[51] T. Tieleman, G. Hinton, Lecture 6.5-rmsprop: divide the gradient by a running average of its recent magnitude, COURSERA: Neural Netw. Mach. Learn. 4 (2) (2012) 26-31.",
        "type": "ListItem"
    },
    {
        "element_id": "e721a4aa31c7b12f50e1eb04677cbcf8",
        "metadata": {
            "file_directory": "paper_for_test",
            "filename": "1-s2.0-S0010482521008313-main.pdf",
            "filetype": "application/pdf"
        },
        "text": "",
        "type": "PageBreak"
    }
]