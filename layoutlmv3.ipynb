{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the json to layoutlv3 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# // ignored_text: 0\n",
    "# // parent_name: 1\n",
    "# // parent_value: 2\n",
    "# // child_key: 3\n",
    "# // child_value: 4\n",
    "label_studio_file_path = \"project-5-at-2024-07-15-09-06-f9dee677.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: standard-settlement-instructions.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_bounding_box(x, y, width, height):\n",
    "\t\"\"\"Converts the given bounding box coordinates to the YOLO format.\n",
    "\n",
    "\tArgs:\n",
    "\tx: The x-coordinate of the top-left corner of the bounding box.\n",
    "\ty: The y-coordinate of the top-left corner of the bounding box.\n",
    "\twidth: The width of the bounding box.\n",
    "\theight: The height of the bounding box.\n",
    "\n",
    "\tReturns:\n",
    "\tA tuple of four coordinates (x1, y1, x2, y2) in the YOLO format.\n",
    "\t\"\"\"\n",
    "\n",
    "\tx1 = x\n",
    "\ty1 = y\n",
    "\tx2 = x + width\n",
    "\ty2 = y + height\n",
    "\n",
    "\treturn [x1, y1, x2, y2]\n",
    "\n",
    "\n",
    "####################################### Loading json data ###################################\n",
    "with open(label_studio_file_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "output = []\n",
    "\n",
    "for annoatated_image in data:\n",
    "\tdata = {}\n",
    "\tannotation = []\n",
    "\tann_list = []\n",
    "\n",
    "\tif len(annoatated_image) < 8:\n",
    "\t\tcontinue\n",
    "\n",
    "\tfor k, v in annoatated_image.items():\n",
    "\t\tif k == 'ocr':\n",
    "\t\t\tv = v.split('8080/')[-1]\n",
    "\t\t\tprint(f'filename: {v}')\n",
    "\n",
    "\t\t\tdata[\"file_name\"] = f\"{label_studio_file_path}/{v}\"\n",
    "\t\t\toutput.append(data)\n",
    "\n",
    "\n",
    "\t\tif k == 'bbox':\n",
    "\t\t\twidth = v[0]['original_width']\n",
    "\t\t\theight = v[0]['original_height']\n",
    "\n",
    "\t\t\tdata[\"height\"] = height\n",
    "\t\t\tdata[\"width\"] = width\n",
    "\n",
    "\n",
    "\tfor bb, text, label in zip(annoatated_image['bbox'], annoatated_image['transcription'],   annoatated_image['label']):\n",
    "\t\tann_dict = {}\n",
    "\n",
    "\t\t# print('text :', text)\n",
    "\n",
    "\t\tann_dict[\"box\"] = convert_bounding_box(bb['x'], bb['y'], bb['width'], bb['height'])\n",
    "\t\tann_dict[\"text\"] = text\n",
    "\t\tann_dict[\"label\"] = label['labels'][-1]\n",
    "\t\tann_list.append(ann_dict)\n",
    "\tdata[\"annotations\"] = ann_list\n",
    "\n",
    "# print(output)\n",
    "with open(\"Training_layoutLMV3.json\", \"w\") as f:\n",
    "  json.dump(output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine code:\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_fn(data_loader, model, optimizer):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)\n",
    "\n",
    "def eval_fn(data_loader, model):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        _, loss = model(**data)\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils code\n",
    "# utils\n",
    "import json\n",
    "\n",
    "def read_json(json_path:str)->dict:\n",
    "    with open(json_path,'r') as fp:\n",
    "        data = json.loads(fp.read())\n",
    "    return data\n",
    "\n",
    "def train_data_format(json_to_dict:list):\n",
    "\n",
    "    final_list = []\n",
    "    count=0\n",
    "    for item in json_to_dict:\n",
    "        count = count+1\n",
    "        # print(item['annotations'])\n",
    "        test_dict = {\"id\":int,\"tokens\":[],\"bboxes\":[],\"ner_tag\":[]}\n",
    "        test_dict[\"id\"] = count\n",
    "        test_dict[\"img_path\"] = item['file_name']\n",
    "        for cont in item['annotations']:\n",
    "            test_dict['tokens'].append(cont['text'])\n",
    "            test_dict['bboxes'].append(cont['box'])\n",
    "            test_dict['ner_tag'].append(cont['label'])\n",
    "\n",
    "        final_list.append(test_dict)\n",
    "    #print(final_list)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "class dataSet:\n",
    "    def __init__(self,json_path,processor=None) -> None:\n",
    "        self.json_data = train_data_format(read_json(json_path))\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self)->int:\n",
    "        # print(self.json_data)\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self,index)->dict:\n",
    "        imgs = []\n",
    "        words = []\n",
    "        label = []\n",
    "        bboxes = []\n",
    "        data = self.json_data[index]\n",
    "        \n",
    "        imgs.append(Image.open(data['img_path']).convert('RGB'))\n",
    "        words.append(data['tokens'])\n",
    "        label.append(data['ner_tag'])\n",
    "        bboxes.append(data['bboxes'])\n",
    "\n",
    "        encoding = self.processor(\n",
    "            imgs,\n",
    "            words,\n",
    "            boxes = bboxes,\n",
    "            word_labels = label,\n",
    "            max_length=512,padding=\"max_length\",truncation=\"longest_first\",return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\" : torch.tensor(encoding[\"input_ids\"],dtype=torch.int64).flatten(),\n",
    "            \"attention_mask\" : torch.tensor(encoding[\"attention_mask\"],dtype=torch.int64).flatten(),\n",
    "            \"bbox\" : torch.tensor(encoding[\"bbox\"],dtype=torch.int64).flatten(end_dim=1),\n",
    "            \"pixel_values\" : torch.tensor(encoding[\"pixel_values\"],dtype=torch.float32).flatten(end_dim=1),\n",
    "            \"lables\" : torch.tensor(encoding[\"labels\"],dtype=torch.int64)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "def loss_fn(pred,target):\n",
    "    return nn.CrossEntropyLoss()(pred.view(-1,4),target.view(-1))\n",
    "\n",
    "class ModelModule(nn.Module):\n",
    "    def __init__(self,n_classes:int) -> None:\n",
    "        super().__init__()\n",
    "        self.model = LayoutLMv3ForTokenClassification.from_pretrained('../inputs/layoutlmv3Microsoft')\n",
    "        self.cls_layer = nn.Sequential(nn.Linear(in_features = 2,\n",
    "                                                out_features = 512),\n",
    "                                      nn.ReLU(), nn.Linear(in_features = 512, out_features = n_classes))\n",
    "\n",
    "    def forward(self,input_ids,attention_mask,bbox,pixel_values,lables=None):\n",
    "        output = self.model(input_ids,attention_mask=attention_mask,bbox=bbox,pixel_values=pixel_values)\n",
    "\n",
    "        output = self.cls_layer(output.logits)\n",
    "\n",
    "        prob = nnf.softmax(output, dim=1)\n",
    "        top_p, top_class = prob.topk(1, dim = 1)\n",
    "\n",
    "        print(\"Probability score :\", prob)\n",
    "        print(\"top_p, top_class \",top_p, top_class)\n",
    "        loss = loss_fn(output,lables)\n",
    "\n",
    "        return  output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "import torch\n",
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "\n",
    "base_model_path = '../inputs/layoutlmv3Microsoft'\n",
    "train_data_path = '../inputs/Training_layoutLMV3.json'\n",
    "save_model_path = '../inputs/model.bin'\n",
    "\n",
    "featur_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "tokeniser = LayoutLMv3TokenizerFast.from_pretrained(base_model_path,ignore_mismatched_sizes=True)\n",
    "\n",
    "processor = LayoutLMv3Processor(tokenizer=tokeniser,feature_extractor=featur_extractor)\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(base_model_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = dataSet(train_data_path,processor)\n",
    "    dataload = torch.utils.data.DataLoader(ds,batch_size=2)\n",
    "\n",
    "    # creating model instance\n",
    "    model = ModelModule(4) \n",
    "\n",
    "    # optimizer and loss\n",
    "    optimizer = AdamW(model.parameters(),lr=5e-5)\n",
    "    best_loss = np.inf\n",
    "\n",
    "    # Training the model\n",
    "    loss_list = []\n",
    "    for epoch in range(30):\n",
    "        # Training\n",
    "        train_loss = train_fn(dataload, model, optimizer)\n",
    "        # print(model.parameters)\n",
    "        # break\n",
    "\n",
    "        if train_loss < best_loss:\n",
    "            torch.save(model.state_dict(), save_model_path)\n",
    "            best_loss = train_loss\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), f'./model_{epoch}.bin')\n",
    "            print('i = {}'.format(epoch))\n",
    "            print(f\"{epoch} with loss {train_loss}\")\n",
    "\n",
    "        print(f\"{epoch} with loss {train_loss}\")\n",
    "\n",
    "        loss_list.append(train_loss)\n",
    "\n",
    "        # evaluation\n",
    "        eval_loss = eval_fn(dataload, model)\n",
    "        print(\"Evaluation loss :\",  eval_loss)\n",
    "\n",
    "    np.array(loss_list).dump(open('loss_list.npy', 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
