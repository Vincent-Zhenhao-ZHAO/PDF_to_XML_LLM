{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PDF to Image, use OCR to detect bounding boxes of text\n",
    "# from pdf2image import convert_from_path\n",
    "# import easyocr\n",
    "# import numpy as np\n",
    "# pdf_path = 'Structured_note_termsheet/cct-linked-structured-note.pdf'\n",
    "# images = convert_from_path(pdf_path)\n",
    "# reader = easyocr.Reader(['en'])\n",
    "# bounds = reader.readtext(np.array(images[0]), min_size=10, slope_ths=0.5, ycenter_ths=0.7, height_ths=1.2, width_ths=0.85, decoder='beamsearch', beamWidth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export the image to a file\n",
    "# images[0].save('output.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86.30400085449219, 280.5127868652344), (526.9910888671875, 340.22802734375))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Get the coordinates bounding box of the text in the PDF\n",
    "# def search_text_and_get_coordinates(pdf_path, search_text):\n",
    "#     document = fitz.open(pdf_path)\n",
    "    \n",
    "#     start_text_coordinates = (float('inf'), float('inf'))\n",
    "#     end_text_coordinates = (float('-inf'), float('-inf'))\n",
    "    \n",
    "#     for page_number in range(len(document)):\n",
    "#         page = document.load_page(page_number)\n",
    "        \n",
    "#         text_instances = page.search_for(search_text)\n",
    "        \n",
    "#         for inst in text_instances:\n",
    "#             x0, y0, x1, y1 = inst\n",
    "#             if start_text_coordinates is None:\n",
    "#                 start_text_coordinates = (x0, y0)\n",
    "#             else:\n",
    "#               # choose the smallest x0 and y0\n",
    "#               start_text_coordinates = (min(start_text_coordinates[0], x0), min(start_text_coordinates[1], y0))\n",
    "        \n",
    "#             if end_text_coordinates is None:\n",
    "#                 end_text_coordinates = (x1, y1)\n",
    "#             else:\n",
    "#               # choose the largest x1 and y1\n",
    "#               end_text_coordinates = (max(end_text_coordinates[0], x1), max(end_text_coordinates[1], y1))\n",
    "              \n",
    "#     return start_text_coordinates, end_text_coordinates\n",
    "\n",
    "# pdf_path = 'Structured_note_termsheet/cct-linked-structured-note.pdf'\n",
    "\n",
    "# search_text = 'Status of the Notes: The Notes are direct, unsecured and unsubordinated obligations of the Issuer and rank pari passu without any preference among themselves and (save for certain obligations required to be preferred by law) equally with all other unsecured obligations (other than subordinated obligations, if any) of the Issuer, from time to time outstanding.'\n",
    "\n",
    "# search_text_and_get_coordinates(pdf_path, search_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json = {\n",
    "#   \"training_data\": [\n",
    "#     {\n",
    "#       \"text\": \"Status of the Notes: The Notes are direct, unsecured and unsubordinated obligations of the Issuer and rank pari passu without any preference among themselves and (save for certain obligations required to be preferred by law) equally with all other unsecured obligations (other than subordinated obligations, if any) of the Issuer, from time to time outstanding.\",\n",
    "#       \"start_text_coordinates\": {\n",
    "#         \"x\": 86.30400085449219,\n",
    "#         \"y\": 280.5127868652344\n",
    "#       },\n",
    "#       \"end_text_coordinates\": {\n",
    "#         \"x\": 526.9910888671875,\n",
    "#         \"y\": 340.22802734375\n",
    "#       },\n",
    "#       \"expected_output\": \"<StatusOfNotes><Description>The Notes are direct, unsecured and unsubordinated obligations of the Issuer and rank pari passu without any preference among themselves and (save for certain obligations required to be preferred by law) equally with all other unsecured obligations (other than subordinated obligations, if any) of the Issuer, from time to time outstanding.</Description></StatusOfNotes>\"\n",
    "#     }\n",
    "#   ]\n",
    "# }\n",
    "\n",
    "# # tabulur data extraction \n",
    "# # key value pairs\n",
    "# # page breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF to Images\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "dpi = 300\n",
    "zoom = dpi/72\n",
    "magnify = fitz.Matrix(zoom, zoom)\n",
    "\n",
    "folder_name = 'standard_settlement_instructions'\n",
    "file_name = 'standard-settlement-instructions.pdf'\n",
    "path = f\"{folder_name}/{file_name}\"\n",
    "count = 0\n",
    "\n",
    "doc = fitz.open(path)\n",
    "\n",
    "for page in doc:\n",
    "    count+=1\n",
    "    pix = page.get_pixmap(matrix=magnify)\n",
    "    pix.save(f\"Images/{file_name[:-3]}_page_{count}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Tesseract OCR engine to extract text from the images\n",
    "import os\n",
    "import json\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "# Set the Tesseract OCR executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'  # Update this path if necessary\n",
    "\n",
    "# Tesseract output levels for the level of detail for the bounding boxes\n",
    "LEVELS = {\n",
    "    'page_num': 1,\n",
    "    'block_num': 2,\n",
    "    'par_num': 3,\n",
    "    'line_num': 4,\n",
    "    'word_num': 5\n",
    "}\n",
    "\n",
    "def create_image_url(filepath):\n",
    "    \"\"\"\n",
    "    Label Studio requires image URLs, so this defines the mapping from filesystem to URLs\n",
    "    if you use ./serve_local_files.sh <my-images-dir>, the image URLs are localhost:8081/filename.png\n",
    "    Otherwise you can build links like /data/upload/filename.png to refer to the files\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "    return f'http://localhost:8080/{filename}'\n",
    "\n",
    "def convert_to_ls(image, tesseract_output, per_level='block_num'):\n",
    "    \"\"\"\n",
    "    :param image: PIL image object\n",
    "    :param tesseract_output: the output from tesseract\n",
    "    :param per_level: control the granularity of bboxes from tesseract\n",
    "    :return: tasks.json ready to be imported into Label Studio with \"Optical Character Recognition\" template\n",
    "    \"\"\"\n",
    "    image_width, image_height = image.size\n",
    "    per_level_idx = LEVELS[per_level]\n",
    "    results = []\n",
    "    all_scores = []\n",
    "    for i, level_idx in enumerate(tesseract_output['level']):\n",
    "        if level_idx == per_level_idx:\n",
    "            bbox = {\n",
    "                'x': 100 * tesseract_output['left'][i] / image_width,\n",
    "                'y': 100 * tesseract_output['top'][i] / image_height,\n",
    "                'width': 100 * tesseract_output['width'][i] / image_width,\n",
    "                'height': 100 * tesseract_output['height'][i] / image_height,\n",
    "                'rotation': 0\n",
    "            }\n",
    "\n",
    "            words, confidences = [], []\n",
    "            for j, curr_id in enumerate(tesseract_output[per_level]):\n",
    "                if curr_id != tesseract_output[per_level][i]:\n",
    "                    continue\n",
    "                word = tesseract_output['text'][j]\n",
    "                confidence = tesseract_output['conf'][j]\n",
    "                words.append(word)\n",
    "                if confidence != '-1':\n",
    "                    confidences.append(float(confidence / 100.))\n",
    "\n",
    "            text = ' '.join(words).strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            region_id = str(uuid4())[:10]\n",
    "            score = sum(confidences) / len(confidences) if confidences else 0\n",
    "            bbox_result = {\n",
    "                'id': region_id, 'from_name': 'bbox', 'to_name': 'image', 'type': 'rectangle',\n",
    "                'value': bbox}\n",
    "            transcription_result = {\n",
    "                'id': region_id, 'from_name': 'transcription', 'to_name': 'image', 'type': 'textarea',\n",
    "                'value': dict(text=[text], **bbox), 'score': score}\n",
    "            results.extend([bbox_result, transcription_result])\n",
    "            all_scores.append(score)\n",
    "\n",
    "    return {\n",
    "        'data': {\n",
    "            'ocr': create_image_url(image.filename)\n",
    "        },\n",
    "        'predictions': [{\n",
    "            'result': results,\n",
    "            'score': sum(all_scores) / len(all_scores) if all_scores else 0\n",
    "        }]\n",
    "    }\n",
    "\n",
    "tasks = []\n",
    "# Collect the receipt images from the image directory\n",
    "for f in Path('Images').glob('*.png'):\n",
    "    with Image.open(f.absolute()) as image:\n",
    "        tesseract_output = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "        task = convert_to_ls(image, tesseract_output, per_level='block_num')\n",
    "        tasks.append(task)\n",
    "\n",
    "# Create a file to import into Label Studio\n",
    "with open(f'JSON/{file_name[:-3]}_noPaddle.json', mode='w') as f:\n",
    "    json.dump(tasks, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar to /Users/vincentzhao/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.00M/4.00M [00:24<00:00, 161kiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/PP-OCRv4/english/en_PP-OCRv4_rec_infer.tar to /Users/vincentzhao/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer/en_PP-OCRv4_rec_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10.2M/10.2M [00:25<00:00, 399kiB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/07/15 07:59:11] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/vincentzhao/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/vincentzhao/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/Users/vincentzhao/Desktop/Project_Master/.conda/lib/python3.11/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='/Users/vincentzhao/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=False, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/07/15 07:59:11] ppocr DEBUG: dt_boxes num : 76, elapsed : 0.27209019660949707\n",
      "[2024/07/15 07:59:20] ppocr DEBUG: rec_res num  : 76, elapsed : 8.070325136184692\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from paddleocr import PaddleOCR\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "\n",
    "# loading the engine\n",
    "# OCR enginer\n",
    "ocr = PaddleOCR(use_angle_cls=False, \n",
    "                lang='en',\n",
    "                  rec=False,\n",
    "                ) # need to run only once to download and load model into memory \n",
    "\n",
    "images_folder_path  = \"Images\" \n",
    "\n",
    "def create_image_url(filename):\n",
    "    \"\"\"\n",
    "    Label Studio requires image URLs, so this defines the mapping from filesystem to URLs\n",
    "    if you use ./serve_local_files.sh <my-images-dir>, the image URLs are localhost:8081/filename.png\n",
    "    Otherwise you can build links like /data/upload/filename.png to refer to the files\n",
    "    \"\"\"\n",
    "    return f'http://localhost:8080/{filename}'\n",
    "\n",
    "def convert_bounding_box(bounding_box):\n",
    "    \"\"\"Converts a bounding box of [x1, y1, x2, y2] into [x, y, height, width].\n",
    "\n",
    "    Args:\n",
    "    bounding_box: A list of four numbers, representing the x1, y1, x2, and y2\n",
    "        coordinates of the bounding box.\n",
    "\n",
    "    Returns:\n",
    "    A list of four numbers, representing the x, y, height, and width of the\n",
    "        bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    x1, y1, x2, y2 = bounding_box\n",
    "    x = min(x1, x2)\n",
    "    y = min(y1, y2)\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    return [x, y, width, height]\n",
    "\n",
    "def extracted_tables_to_label_studio_json_file_with_paddleOCR(images_folder_path):\n",
    "    label_studio_task_list = []\n",
    "    for images in os.listdir(images_folder_path):\n",
    "        if images.endswith('.png'):\n",
    "            output_json = {}\n",
    "            annotation_result = []\n",
    "\n",
    "            output_json['data'] =  {\"ocr\":create_image_url(images)}\n",
    "                    \n",
    "            img = Image.open(f'Images/{images}')\n",
    "\n",
    "            img = np.asarray(img)\n",
    "            image_height, image_width = img.shape[:2]\n",
    "\n",
    "            result = ocr.ocr(img,cls=False)\n",
    "\n",
    "            #print(result)\n",
    "\n",
    "            for output in result:\n",
    "\n",
    "                for item in output: \n",
    "                    co_ord = item[0]\n",
    "                    text = item[1][0]\n",
    "\n",
    "                    four_co_ord = [co_ord[0][0],co_ord[1][1],co_ord[2][0]-co_ord[0][0],co_ord[2][1]-co_ord[1][1]]\n",
    "\n",
    "                    #print(four_co_ord)\n",
    "                    #print(text)\n",
    "\n",
    "                    bbox = {\n",
    "                    'x': 100 * four_co_ord[0] / image_width,\n",
    "                    'y': 100 * four_co_ord[1] / image_height,\n",
    "                    'width': 100 * four_co_ord[2] / image_width,\n",
    "                    'height': 100 * four_co_ord[3] / image_height,\n",
    "                    'rotation': 0\n",
    "                            }\n",
    "                    \n",
    "\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    region_id = str(uuid4())[:10]\n",
    "                    score = 0.5\n",
    "                    bbox_result = {\n",
    "                        'id': region_id, 'from_name': 'bbox', 'to_name': 'image', 'type': 'rectangle',\n",
    "                        'value': bbox}\n",
    "                    transcription_result = {\n",
    "                        'id': region_id, 'from_name': 'transcription', 'to_name': 'image', 'type': 'textarea',\n",
    "                        'value': dict(text=[text], **bbox), 'score': score}\n",
    "                    annotation_result.extend([bbox_result, transcription_result])\n",
    "                    #print('annotation_result :\\n',annotation_result)\n",
    "            output_json['predictions'] = [ {\"result\": annotation_result,  \"score\":0.97}]\n",
    "\n",
    "            label_studio_task_list.append(output_json)\n",
    "        \n",
    "    # saving label_stdui_task_list as json file to import in label_studio\n",
    "    with open(f'JSON/{file_name[:-3]}_Paddle.json', 'w') as f:\n",
    "        json.dump(label_studio_task_list, f, indent=4)\n",
    "\n",
    "extracted_tables_to_label_studio_json_file_with_paddleOCR(images_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincentzhao/Desktop/Project_Master/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf \n",
    "from unstructured.staging.base import elements_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_page_breaks = True\n",
    "\n",
    "## strategy\n",
    "# The strategy to use for partitioning the PDF. Valid strategies are \"hi_res\", \"ocr_only\", and \"fast\".\n",
    "# When using the \"hi_res\" strategy, the function uses a layout detection model to identify document elements.\n",
    "# hi_res\" is used for analyzing PDFs and extracting table structure (default is \"auto\")\n",
    "strategy = \"hi_res\"\n",
    "\n",
    "if strategy == \"hi_res\": infer_table_structure = True\n",
    "else: infer_table_structure = False\n",
    "\n",
    "if infer_table_structure == True: extract_element_types=['Table']\n",
    "else: extract_element_types=None\n",
    "\n",
    "if strategy != \"ocr_only\": max_characters = None\n",
    "\n",
    "languages = [\"eng\"] # example if more than one \"eng+por\" (default is \"eng\")\n",
    "\n",
    "## model_name\n",
    "# @requires_dependencies(\"unstructured_inference\")\n",
    "# yolox: best model for table extraction. Other options are yolox_quantized, detectron2_onnx and chipper depending on file layout\n",
    "# source: https://unstructured-io.github.io/unstructured/best_practices/models.html\n",
    "hi_res_model_name = \"yolox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirement.txt:\n",
    "# pillow\n",
    "# unstructured[all-docs]==0.12.5\n",
    "# pdfminer.six==20201018\n",
    "# pillow-heif\n",
    "# unstructured_inference\n",
    "# opencv-python-headless\n",
    "# opencv-python\n",
    "# unstructured_pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "filename = \"standard_settlement_instructions/wfbna-usa-ssi-listing.pdf\"\n",
    "\n",
    "elements = partition_pdf(\n",
    "        filename=filename,\n",
    "        include_page_breaks=include_page_breaks,\n",
    "        strategy=strategy,\n",
    "        infer_table_structure=infer_table_structure,\n",
    "        extract_element_types=extract_element_types,\n",
    "        max_characters=max_characters,\n",
    "        languages=languages,\n",
    "        hi_res_model_name=hi_res_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.staging.base import elements_to_json\n",
    "elements_to_json(elements, filename=f\"{filename}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "def process_json_file(input_filename):\n",
    "    # Read the JSON file\n",
    "    with open(input_filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Iterate over the JSON data and extract required table elements\n",
    "    extracted_elements = []\n",
    "    text_prev = \"\"\n",
    "    for i,entry in enumerate(data):\n",
    "        if entry[\"type\"] == \"Title\":\n",
    "            text = \"\" + entry[\"text\"] + \"\"\n",
    "        elif entry[\"type\"] == \"Table\":\n",
    "            text = entry[\"metadata\"][\"text_as_html\"]\n",
    "        else:\n",
    "            text = \"\" + entry[\"text\"] + \"\"\n",
    "\n",
    "        if text != text_prev: extracted_elements.append(text)\n",
    "        text_prev = text\n",
    "\n",
    "    # Write the extracted elements to the output file\n",
    "    html_start = \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    Document Information\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    html_end = \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    output_file_html = Path(input_filename).name.replace(\".json\", \"\") + \".html\"\n",
    "    with open(output_file_html, 'w') as output_file:\n",
    "        output_file.write(html_start + \"\\n\")\n",
    "        for element in extracted_elements:\n",
    "            output_file.write(element + \"\\n\")\n",
    "        output_file.write(html_end + \"\\n\")\n",
    "\n",
    "    return str(output_file_html)\n",
    "\n",
    "output_file_html = process_json_file(f\"{filename}.json\") # It can take a while for the .html file to show up in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use llama to convert the HTML file to a xml file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
